{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AfvmKV6FzQgG",
    "outputId": "d7a0e465-3dd8-49b6-e17a-b896f47ca30b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwvO76hCzQgI",
    "outputId": "f53ac91c-d263-4182-8bcf-0b841e7c394c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 26421880/26421880 [00:00<00:00, 115762124.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 4620763.78it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "100%|██████████| 4422102/4422102 [00:00<00:00, 60937806.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 12459478.93it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9srIobr8zQgJ",
    "outputId": "2f129bf2-8811-421d-8b9f-242f24a171eb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# from: https://www.kaggle.com/code/pankajj/fashion-mnist-with-pytorch-93-accuracy\n",
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "                 0: \"T-shirt/Top\",\n",
    "                 1: \"Trouser\",\n",
    "                 2: \"Pullover\",\n",
    "                 3: \"Dress\",\n",
    "                 4: \"Coat\",\n",
    "                 5: \"Sandal\",\n",
    "                 6: \"Shirt\",\n",
    "                 7: \"Sneaker\",\n",
    "                 8: \"Bag\",\n",
    "                 9: \"Ankle Boot\"\n",
    "                 }\n",
    "    input = (label.item() if type(label) == torch.Tensor else label)\n",
    "    return output_mapping[input]"
   ],
   "metadata": {
    "id": "TYlpD1XwzQgJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Feed Forward Neural Network\n",
    "\n",
    "# model architecture from: https://www.kaggle.com/code/pankajj/fashion-mnist-with-pytorch-93-accuracy\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_shape : torch.Size, hidden_sizes : list[int], num_classes : int, activation_function, loss_fn):\n",
    "        super().__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            activation_function,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            activation_function,\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential( # todo use hidden sizes\n",
    "            nn.Linear(64*6*6, 600),\n",
    "            nn.Dropout(0.25),\n",
    "            activation_function,\n",
    "            nn.Linear(600, 120),\n",
    "            activation_function,\n",
    "            nn.Linear(120, num_classes) # no softmax necessary\n",
    "        )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def train_network(self, dataloader : DataLoader, optimizer : torch.optim.Optimizer):\n",
    "        size = len(dataloader.dataset)\n",
    "        self.train()\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.loss_fn(y_pred, F.one_hot(y, self.num_classes).type(y_pred.dtype))\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    def test(self, dataloader : DataLoader):\n",
    "        num_batches = len(dataloader)\n",
    "        self.eval()\n",
    "        test_loss = 0\n",
    "        num_right = 0\n",
    "        num_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_pred  = self.forward(X)\n",
    "                test_loss += self.loss_fn(y_pred , F.one_hot(y, self.num_classes).type(y_pred.dtype)).item()\n",
    "                num_correct += (torch.argmax(y_pred, dim=1) == y).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        accuracy = num_correct / len(dataloader.dataset)\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f} Accuracy: {float(accuracy):>8f} \\n\")"
   ],
   "metadata": {
    "id": "67ZbMpwqzQgK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([2, 1, 28, 28])\n",
      "Shape of y: torch.Size([2]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ajSSZlMzQgL",
    "outputId": "5d18c304-18de-4a42-b4d2-a4612efcfd05"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.236966  [    2/60000]\n",
      "loss: 3.268069  [  202/60000]\n",
      "loss: 1.389572  [  402/60000]\n",
      "loss: 0.654887  [  602/60000]\n",
      "loss: 1.613296  [  802/60000]\n",
      "loss: 0.460688  [ 1002/60000]\n",
      "loss: 0.743810  [ 1202/60000]\n",
      "loss: 2.145666  [ 1402/60000]\n",
      "loss: 3.367494  [ 1602/60000]\n",
      "loss: 0.362263  [ 1802/60000]\n",
      "loss: 0.277623  [ 2002/60000]\n",
      "loss: 0.056401  [ 2202/60000]\n",
      "loss: 2.257266  [ 2402/60000]\n",
      "loss: 1.535370  [ 2602/60000]\n",
      "loss: 0.878837  [ 2802/60000]\n",
      "loss: 0.625087  [ 3002/60000]\n",
      "loss: 0.574371  [ 3202/60000]\n",
      "loss: 0.631517  [ 3402/60000]\n",
      "loss: 1.644186  [ 3602/60000]\n",
      "loss: 0.317648  [ 3802/60000]\n",
      "loss: 0.149533  [ 4002/60000]\n",
      "loss: 0.005241  [ 4202/60000]\n",
      "loss: 0.983753  [ 4402/60000]\n",
      "loss: 0.000120  [ 4602/60000]\n",
      "loss: 0.870985  [ 4802/60000]\n",
      "loss: 0.336335  [ 5002/60000]\n",
      "loss: 0.104162  [ 5202/60000]\n",
      "loss: 0.297630  [ 5402/60000]\n",
      "loss: 0.655000  [ 5602/60000]\n",
      "loss: 0.034855  [ 5802/60000]\n",
      "loss: 0.776370  [ 6002/60000]\n",
      "loss: 0.000382  [ 6202/60000]\n",
      "loss: 0.218551  [ 6402/60000]\n",
      "loss: 0.016396  [ 6602/60000]\n",
      "loss: 0.001037  [ 6802/60000]\n",
      "loss: 0.855328  [ 7002/60000]\n",
      "loss: 2.287640  [ 7202/60000]\n",
      "loss: 0.863094  [ 7402/60000]\n",
      "loss: 1.093819  [ 7602/60000]\n",
      "loss: 1.089002  [ 7802/60000]\n",
      "loss: 0.283379  [ 8002/60000]\n",
      "loss: 0.001544  [ 8202/60000]\n",
      "loss: 0.051139  [ 8402/60000]\n",
      "loss: 0.370929  [ 8602/60000]\n",
      "loss: 0.044374  [ 8802/60000]\n",
      "loss: 0.071263  [ 9002/60000]\n",
      "loss: 0.375716  [ 9202/60000]\n",
      "loss: 0.018179  [ 9402/60000]\n",
      "loss: 0.808919  [ 9602/60000]\n",
      "loss: 0.375408  [ 9802/60000]\n",
      "loss: 0.008656  [10002/60000]\n",
      "loss: 0.414466  [10202/60000]\n",
      "loss: 0.562156  [10402/60000]\n",
      "loss: 0.377525  [10602/60000]\n",
      "loss: 0.438117  [10802/60000]\n",
      "loss: 0.005496  [11002/60000]\n",
      "loss: 0.127769  [11202/60000]\n",
      "loss: 0.002397  [11402/60000]\n",
      "loss: 0.023026  [11602/60000]\n",
      "loss: 0.001082  [11802/60000]\n",
      "loss: 0.509252  [12002/60000]\n",
      "loss: 0.486047  [12202/60000]\n",
      "loss: 0.635868  [12402/60000]\n",
      "loss: 0.001074  [12602/60000]\n",
      "loss: 0.002047  [12802/60000]\n",
      "loss: 0.008821  [13002/60000]\n",
      "loss: 0.313857  [13202/60000]\n",
      "loss: 1.446695  [13402/60000]\n",
      "loss: 0.913494  [13602/60000]\n",
      "loss: 0.019592  [13802/60000]\n",
      "loss: 0.019410  [14002/60000]\n",
      "loss: 0.865982  [14202/60000]\n",
      "loss: 0.264955  [14402/60000]\n",
      "loss: 0.262211  [14602/60000]\n",
      "loss: 0.512067  [14802/60000]\n",
      "loss: 0.679166  [15002/60000]\n",
      "loss: 0.022313  [15202/60000]\n",
      "loss: 0.016648  [15402/60000]\n",
      "loss: 0.018685  [15602/60000]\n",
      "loss: 0.096191  [15802/60000]\n",
      "loss: 1.970866  [16002/60000]\n",
      "loss: 0.152253  [16202/60000]\n",
      "loss: 1.496716  [16402/60000]\n",
      "loss: 0.195675  [16602/60000]\n",
      "loss: 0.429975  [16802/60000]\n",
      "loss: 0.069732  [17002/60000]\n",
      "loss: 0.061602  [17202/60000]\n",
      "loss: 0.015540  [17402/60000]\n",
      "loss: 0.028091  [17602/60000]\n",
      "loss: 0.125176  [17802/60000]\n",
      "loss: 0.489372  [18002/60000]\n",
      "loss: 1.218921  [18202/60000]\n",
      "loss: 0.008782  [18402/60000]\n",
      "loss: 0.034969  [18602/60000]\n",
      "loss: 0.373474  [18802/60000]\n",
      "loss: 0.618431  [19002/60000]\n",
      "loss: 0.071745  [19202/60000]\n",
      "loss: 1.027176  [19402/60000]\n",
      "loss: 0.087124  [19602/60000]\n",
      "loss: 0.372099  [19802/60000]\n",
      "loss: 0.002139  [20002/60000]\n",
      "loss: 0.726305  [20202/60000]\n",
      "loss: 0.430037  [20402/60000]\n",
      "loss: 0.817983  [20602/60000]\n",
      "loss: 1.178620  [20802/60000]\n",
      "loss: 0.036557  [21002/60000]\n",
      "loss: 0.000152  [21202/60000]\n",
      "loss: 0.448642  [21402/60000]\n",
      "loss: 0.827212  [21602/60000]\n",
      "loss: 0.958741  [21802/60000]\n",
      "loss: 0.105888  [22002/60000]\n",
      "loss: 0.010397  [22202/60000]\n",
      "loss: 0.036118  [22402/60000]\n",
      "loss: 0.000476  [22602/60000]\n",
      "loss: 0.226971  [22802/60000]\n",
      "loss: 0.462659  [23002/60000]\n",
      "loss: 0.854055  [23202/60000]\n",
      "loss: 1.425571  [23402/60000]\n",
      "loss: 0.000026  [23602/60000]\n",
      "loss: 0.090152  [23802/60000]\n",
      "loss: 0.218886  [24002/60000]\n",
      "loss: 0.000161  [24202/60000]\n",
      "loss: 0.165153  [24402/60000]\n",
      "loss: 0.342603  [24602/60000]\n",
      "loss: 0.004510  [24802/60000]\n",
      "loss: 0.032722  [25002/60000]\n",
      "loss: 0.000001  [25202/60000]\n",
      "loss: 0.954450  [25402/60000]\n",
      "loss: 1.500089  [25602/60000]\n",
      "loss: 0.711614  [25802/60000]\n",
      "loss: 1.333332  [26002/60000]\n",
      "loss: 0.390985  [26202/60000]\n",
      "loss: 0.180498  [26402/60000]\n",
      "loss: 0.365094  [26602/60000]\n",
      "loss: 0.000232  [26802/60000]\n",
      "loss: 0.007474  [27002/60000]\n",
      "loss: 0.001041  [27202/60000]\n",
      "loss: 0.031250  [27402/60000]\n",
      "loss: 1.382373  [27602/60000]\n",
      "loss: 0.667820  [27802/60000]\n",
      "loss: 0.491806  [28002/60000]\n",
      "loss: 0.021590  [28202/60000]\n",
      "loss: 0.064427  [28402/60000]\n",
      "loss: 0.017871  [28602/60000]\n",
      "loss: 0.000262  [28802/60000]\n",
      "loss: 0.495438  [29002/60000]\n",
      "loss: 0.000009  [29202/60000]\n",
      "loss: 0.005002  [29402/60000]\n",
      "loss: 0.082344  [29602/60000]\n",
      "loss: 0.628209  [29802/60000]\n",
      "loss: 0.032308  [30002/60000]\n",
      "loss: 0.894503  [30202/60000]\n",
      "loss: 0.000237  [30402/60000]\n",
      "loss: 0.015720  [30602/60000]\n",
      "loss: 0.002296  [30802/60000]\n",
      "loss: 0.000292  [31002/60000]\n",
      "loss: 0.062389  [31202/60000]\n",
      "loss: 0.048008  [31402/60000]\n",
      "loss: 0.341393  [31602/60000]\n",
      "loss: 0.000789  [31802/60000]\n",
      "loss: 0.657264  [32002/60000]\n",
      "loss: 0.352837  [32202/60000]\n",
      "loss: 0.004364  [32402/60000]\n",
      "loss: 0.772799  [32602/60000]\n",
      "loss: 0.428551  [32802/60000]\n",
      "loss: 0.038549  [33002/60000]\n",
      "loss: 1.137371  [33202/60000]\n",
      "loss: 0.166103  [33402/60000]\n",
      "loss: 0.122030  [33602/60000]\n",
      "loss: 0.648661  [33802/60000]\n",
      "loss: 0.250024  [34002/60000]\n",
      "loss: 0.744179  [34202/60000]\n",
      "loss: 0.917347  [34402/60000]\n",
      "loss: 0.057695  [34602/60000]\n",
      "loss: -0.000000  [34802/60000]\n",
      "loss: 0.946138  [35002/60000]\n",
      "loss: 0.544636  [35202/60000]\n",
      "loss: 0.673004  [35402/60000]\n",
      "loss: 0.000005  [35602/60000]\n",
      "loss: 0.735951  [35802/60000]\n",
      "loss: 0.073011  [36002/60000]\n",
      "loss: 0.424918  [36202/60000]\n",
      "loss: 0.029767  [36402/60000]\n",
      "loss: 0.000382  [36602/60000]\n",
      "loss: 0.014831  [36802/60000]\n",
      "loss: 0.119624  [37002/60000]\n",
      "loss: 0.020671  [37202/60000]\n",
      "loss: 0.094018  [37402/60000]\n",
      "loss: 0.455340  [37602/60000]\n",
      "loss: 0.408752  [37802/60000]\n",
      "loss: 0.284211  [38002/60000]\n",
      "loss: 0.011785  [38202/60000]\n",
      "loss: 0.000070  [38402/60000]\n",
      "loss: 0.246743  [38602/60000]\n",
      "loss: 0.311718  [38802/60000]\n",
      "loss: 0.074775  [39002/60000]\n",
      "loss: 0.004416  [39202/60000]\n",
      "loss: 0.638229  [39402/60000]\n",
      "loss: 0.322922  [39602/60000]\n",
      "loss: 0.000007  [39802/60000]\n",
      "loss: 0.000873  [40002/60000]\n",
      "loss: 0.154297  [40202/60000]\n",
      "loss: 0.030757  [40402/60000]\n",
      "loss: 0.046295  [40602/60000]\n",
      "loss: 0.369546  [40802/60000]\n",
      "loss: 0.009165  [41002/60000]\n",
      "loss: 0.216825  [41202/60000]\n",
      "loss: 0.118209  [41402/60000]\n",
      "loss: 0.067439  [41602/60000]\n",
      "loss: 0.062847  [41802/60000]\n",
      "loss: 0.316078  [42002/60000]\n",
      "loss: 0.014748  [42202/60000]\n",
      "loss: 0.014827  [42402/60000]\n",
      "loss: 0.304726  [42602/60000]\n",
      "loss: 0.052039  [42802/60000]\n",
      "loss: 0.613152  [43002/60000]\n",
      "loss: 0.004674  [43202/60000]\n",
      "loss: 0.000000  [43402/60000]\n",
      "loss: 0.020380  [43602/60000]\n",
      "loss: 0.385954  [43802/60000]\n",
      "loss: 0.000014  [44002/60000]\n",
      "loss: 0.086016  [44202/60000]\n",
      "loss: 0.014908  [44402/60000]\n",
      "loss: 0.144877  [44602/60000]\n",
      "loss: 0.000096  [44802/60000]\n",
      "loss: 0.250439  [45002/60000]\n",
      "loss: 0.240105  [45202/60000]\n",
      "loss: 0.097787  [45402/60000]\n",
      "loss: 0.000048  [45602/60000]\n",
      "loss: 0.445152  [45802/60000]\n",
      "loss: 0.130251  [46002/60000]\n",
      "loss: 0.014379  [46202/60000]\n",
      "loss: 0.338359  [46402/60000]\n",
      "loss: 0.766142  [46602/60000]\n",
      "loss: 0.041838  [46802/60000]\n",
      "loss: 0.061300  [47002/60000]\n",
      "loss: 0.040007  [47202/60000]\n",
      "loss: 0.108895  [47402/60000]\n",
      "loss: 0.534498  [47602/60000]\n",
      "loss: 0.156750  [47802/60000]\n",
      "loss: 0.000192  [48002/60000]\n",
      "loss: 0.134483  [48202/60000]\n",
      "loss: 0.007423  [48402/60000]\n",
      "loss: 0.388286  [48602/60000]\n",
      "loss: 0.000012  [48802/60000]\n",
      "loss: 0.084442  [49002/60000]\n",
      "loss: 0.041481  [49202/60000]\n",
      "loss: 0.001003  [49402/60000]\n",
      "loss: 0.023226  [49602/60000]\n",
      "loss: 0.343754  [49802/60000]\n",
      "loss: 0.036640  [50002/60000]\n",
      "loss: 0.002707  [50202/60000]\n",
      "loss: 0.002909  [50402/60000]\n",
      "loss: 0.327346  [50602/60000]\n",
      "loss: 0.268072  [50802/60000]\n",
      "loss: 0.054306  [51002/60000]\n",
      "loss: 0.000081  [51202/60000]\n",
      "loss: 0.376184  [51402/60000]\n",
      "loss: 3.349450  [51602/60000]\n",
      "loss: 0.030828  [51802/60000]\n",
      "loss: 0.000363  [52002/60000]\n",
      "loss: 0.002648  [52202/60000]\n",
      "loss: 0.390687  [52402/60000]\n",
      "loss: 0.506264  [52602/60000]\n",
      "loss: 0.040243  [52802/60000]\n",
      "loss: 0.347000  [53002/60000]\n",
      "loss: 0.357492  [53202/60000]\n",
      "loss: 0.575394  [53402/60000]\n",
      "loss: 0.047908  [53602/60000]\n",
      "loss: 0.109974  [53802/60000]\n",
      "loss: 0.143448  [54002/60000]\n",
      "loss: 0.708421  [54202/60000]\n",
      "loss: 0.355868  [54402/60000]\n",
      "loss: 0.721416  [54602/60000]\n",
      "loss: 1.599900  [54802/60000]\n",
      "loss: 1.528544  [55002/60000]\n",
      "loss: 0.000000  [55202/60000]\n",
      "loss: 0.055463  [55402/60000]\n",
      "loss: 0.039632  [55602/60000]\n",
      "loss: 0.400857  [55802/60000]\n",
      "loss: 0.023059  [56002/60000]\n",
      "loss: 0.130957  [56202/60000]\n",
      "loss: 0.066381  [56402/60000]\n",
      "loss: 1.584077  [56602/60000]\n",
      "loss: 0.000328  [56802/60000]\n",
      "loss: 0.067099  [57002/60000]\n",
      "loss: 0.199855  [57202/60000]\n",
      "loss: 0.475159  [57402/60000]\n",
      "loss: 2.604058  [57602/60000]\n",
      "loss: 0.408494  [57802/60000]\n",
      "loss: 0.000391  [58002/60000]\n",
      "loss: 0.010346  [58202/60000]\n",
      "loss: 0.082791  [58402/60000]\n",
      "loss: 0.130265  [58602/60000]\n",
      "loss: 0.910717  [58802/60000]\n",
      "loss: 0.000378  [59002/60000]\n",
      "loss: 0.639785  [59202/60000]\n",
      "loss: 0.697497  [59402/60000]\n",
      "loss: 0.039081  [59602/60000]\n",
      "loss: 0.602069  [59802/60000]\n",
      "Test Error: Avg loss: 0.355915 Accuracy: 0.875800 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.005602  [    2/60000]\n",
      "loss: 0.125503  [  202/60000]\n",
      "loss: 0.000204  [  402/60000]\n",
      "loss: 0.831974  [  602/60000]\n",
      "loss: 0.005405  [  802/60000]\n",
      "loss: 0.020794  [ 1002/60000]\n",
      "loss: 0.248403  [ 1202/60000]\n",
      "loss: 0.043293  [ 1402/60000]\n",
      "loss: 2.971227  [ 1602/60000]\n",
      "loss: 0.214436  [ 1802/60000]\n",
      "loss: 0.210702  [ 2002/60000]\n",
      "loss: 0.000192  [ 2202/60000]\n",
      "loss: 0.052444  [ 2402/60000]\n",
      "loss: 0.352337  [ 2602/60000]\n",
      "loss: 0.004948  [ 2802/60000]\n",
      "loss: 0.082449  [ 3002/60000]\n",
      "loss: 0.382036  [ 3202/60000]\n",
      "loss: 0.724571  [ 3402/60000]\n",
      "loss: 0.019450  [ 3602/60000]\n",
      "loss: 0.021785  [ 3802/60000]\n",
      "loss: 0.002166  [ 4002/60000]\n",
      "loss: 0.000884  [ 4202/60000]\n",
      "loss: 0.111378  [ 4402/60000]\n",
      "loss: 0.000008  [ 4602/60000]\n",
      "loss: 1.387975  [ 4802/60000]\n",
      "loss: 0.024133  [ 5002/60000]\n",
      "loss: 0.014958  [ 5202/60000]\n",
      "loss: 0.200955  [ 5402/60000]\n",
      "loss: 0.253525  [ 5602/60000]\n",
      "loss: 0.066407  [ 5802/60000]\n",
      "loss: 0.267868  [ 6002/60000]\n",
      "loss: 0.000118  [ 6202/60000]\n",
      "loss: 0.180760  [ 6402/60000]\n",
      "loss: 0.015235  [ 6602/60000]\n",
      "loss: 0.000476  [ 6802/60000]\n",
      "loss: 0.690963  [ 7002/60000]\n",
      "loss: 3.217078  [ 7202/60000]\n",
      "loss: 0.639946  [ 7402/60000]\n",
      "loss: 1.258575  [ 7602/60000]\n",
      "loss: 0.417203  [ 7802/60000]\n",
      "loss: 0.000002  [ 8002/60000]\n",
      "loss: 0.011691  [ 8202/60000]\n",
      "loss: 0.044162  [ 8402/60000]\n",
      "loss: 0.216017  [ 8602/60000]\n",
      "loss: 0.000001  [ 8802/60000]\n",
      "loss: 0.004245  [ 9002/60000]\n",
      "loss: 0.276915  [ 9202/60000]\n",
      "loss: 0.009143  [ 9402/60000]\n",
      "loss: 0.002435  [ 9602/60000]\n",
      "loss: 0.307015  [ 9802/60000]\n",
      "loss: 0.006674  [10002/60000]\n",
      "loss: 0.166440  [10202/60000]\n",
      "loss: 1.216790  [10402/60000]\n",
      "loss: 0.027998  [10602/60000]\n",
      "loss: 0.324850  [10802/60000]\n",
      "loss: 0.017996  [11002/60000]\n",
      "loss: 0.060926  [11202/60000]\n",
      "loss: 0.000001  [11402/60000]\n",
      "loss: 0.122682  [11602/60000]\n",
      "loss: 0.000637  [11802/60000]\n",
      "loss: 0.320178  [12002/60000]\n",
      "loss: 0.374262  [12202/60000]\n",
      "loss: 0.020875  [12402/60000]\n",
      "loss: 0.000020  [12602/60000]\n",
      "loss: 0.000018  [12802/60000]\n",
      "loss: 0.007538  [13002/60000]\n",
      "loss: 0.468081  [13202/60000]\n",
      "loss: 1.049227  [13402/60000]\n",
      "loss: 0.241976  [13602/60000]\n",
      "loss: 0.097833  [13802/60000]\n",
      "loss: 0.000644  [14002/60000]\n",
      "loss: 0.813487  [14202/60000]\n",
      "loss: 0.015939  [14402/60000]\n",
      "loss: 0.066195  [14602/60000]\n",
      "loss: 0.384032  [14802/60000]\n",
      "loss: 0.141379  [15002/60000]\n",
      "loss: 0.001282  [15202/60000]\n",
      "loss: -0.000000  [15402/60000]\n",
      "loss: 0.000284  [15602/60000]\n",
      "loss: 0.000371  [15802/60000]\n",
      "loss: 2.004411  [16002/60000]\n",
      "loss: 0.393579  [16202/60000]\n",
      "loss: 0.056255  [16402/60000]\n",
      "loss: 0.044109  [16602/60000]\n",
      "loss: 0.880076  [16802/60000]\n",
      "loss: 0.101525  [17002/60000]\n",
      "loss: 0.001616  [17202/60000]\n",
      "loss: 0.163875  [17402/60000]\n",
      "loss: 0.051642  [17602/60000]\n",
      "loss: 0.176409  [17802/60000]\n",
      "loss: 0.060634  [18002/60000]\n",
      "loss: 0.265287  [18202/60000]\n",
      "loss: 0.109121  [18402/60000]\n",
      "loss: 0.000002  [18602/60000]\n",
      "loss: 0.539415  [18802/60000]\n",
      "loss: 0.526879  [19002/60000]\n",
      "loss: 0.102454  [19202/60000]\n",
      "loss: 0.115409  [19402/60000]\n",
      "loss: 0.039624  [19602/60000]\n",
      "loss: 0.163124  [19802/60000]\n",
      "loss: 0.000084  [20002/60000]\n",
      "loss: 0.213996  [20202/60000]\n",
      "loss: 0.440827  [20402/60000]\n",
      "loss: 0.569326  [20602/60000]\n",
      "loss: 1.913112  [20802/60000]\n",
      "loss: 0.042121  [21002/60000]\n",
      "loss: 0.000051  [21202/60000]\n",
      "loss: 0.135724  [21402/60000]\n",
      "loss: 0.434795  [21602/60000]\n",
      "loss: 0.675791  [21802/60000]\n",
      "loss: 0.017841  [22002/60000]\n",
      "loss: 0.004526  [22202/60000]\n",
      "loss: 0.073736  [22402/60000]\n",
      "loss: 0.000078  [22602/60000]\n",
      "loss: 0.063126  [22802/60000]\n",
      "loss: 0.157133  [23002/60000]\n",
      "loss: 0.908380  [23202/60000]\n",
      "loss: 2.102061  [23402/60000]\n",
      "loss: 0.000004  [23602/60000]\n",
      "loss: 0.028058  [23802/60000]\n",
      "loss: 0.015049  [24002/60000]\n",
      "loss: 0.000090  [24202/60000]\n",
      "loss: 0.074628  [24402/60000]\n",
      "loss: 0.432244  [24602/60000]\n",
      "loss: 0.001735  [24802/60000]\n",
      "loss: 0.016476  [25002/60000]\n",
      "loss: 0.000009  [25202/60000]\n",
      "loss: 0.424957  [25402/60000]\n",
      "loss: 0.925833  [25602/60000]\n",
      "loss: 1.550669  [25802/60000]\n",
      "loss: 0.720714  [26002/60000]\n",
      "loss: 0.404656  [26202/60000]\n",
      "loss: 0.206107  [26402/60000]\n",
      "loss: 0.337835  [26602/60000]\n",
      "loss: -0.000000  [26802/60000]\n",
      "loss: 0.000063  [27002/60000]\n",
      "loss: 0.005363  [27202/60000]\n",
      "loss: 0.075039  [27402/60000]\n",
      "loss: 0.400342  [27602/60000]\n",
      "loss: 1.098555  [27802/60000]\n",
      "loss: 0.171003  [28002/60000]\n",
      "loss: -0.000000  [28202/60000]\n",
      "loss: 0.155157  [28402/60000]\n",
      "loss: 0.029029  [28602/60000]\n",
      "loss: 0.000004  [28802/60000]\n",
      "loss: 0.022123  [29002/60000]\n",
      "loss: 0.000003  [29202/60000]\n",
      "loss: 0.004006  [29402/60000]\n",
      "loss: 0.153968  [29602/60000]\n",
      "loss: 0.899571  [29802/60000]\n",
      "loss: 0.002477  [30002/60000]\n",
      "loss: 0.552442  [30202/60000]\n",
      "loss: 0.000235  [30402/60000]\n",
      "loss: 0.004428  [30602/60000]\n",
      "loss: 0.000050  [30802/60000]\n",
      "loss: 0.000004  [31002/60000]\n",
      "loss: 0.096011  [31202/60000]\n",
      "loss: 0.041420  [31402/60000]\n",
      "loss: 0.389093  [31602/60000]\n",
      "loss: 0.047368  [31802/60000]\n",
      "loss: 1.014390  [32002/60000]\n",
      "loss: 0.335346  [32202/60000]\n",
      "loss: 0.000005  [32402/60000]\n",
      "loss: 0.541155  [32602/60000]\n",
      "loss: 0.304633  [32802/60000]\n",
      "loss: 0.000204  [33002/60000]\n",
      "loss: 0.993737  [33202/60000]\n",
      "loss: 0.079085  [33402/60000]\n",
      "loss: 0.101171  [33602/60000]\n",
      "loss: 0.563853  [33802/60000]\n",
      "loss: 0.005609  [34002/60000]\n",
      "loss: 0.614468  [34202/60000]\n",
      "loss: 1.000994  [34402/60000]\n",
      "loss: 0.025987  [34602/60000]\n",
      "loss: -0.000000  [34802/60000]\n",
      "loss: 0.254645  [35002/60000]\n",
      "loss: 0.217425  [35202/60000]\n",
      "loss: 0.235303  [35402/60000]\n",
      "loss: 0.000000  [35602/60000]\n",
      "loss: 0.393006  [35802/60000]\n",
      "loss: 0.039895  [36002/60000]\n",
      "loss: 0.458570  [36202/60000]\n",
      "loss: 0.005598  [36402/60000]\n",
      "loss: 0.001330  [36602/60000]\n",
      "loss: 0.030882  [36802/60000]\n",
      "loss: 0.008857  [37002/60000]\n",
      "loss: 0.011044  [37202/60000]\n",
      "loss: 0.015137  [37402/60000]\n",
      "loss: 0.636053  [37602/60000]\n",
      "loss: 0.347839  [37802/60000]\n",
      "loss: 0.196486  [38002/60000]\n",
      "loss: 0.006390  [38202/60000]\n",
      "loss: 0.000140  [38402/60000]\n",
      "loss: 0.034819  [38602/60000]\n",
      "loss: 0.088072  [38802/60000]\n",
      "loss: 0.042182  [39002/60000]\n",
      "loss: 0.000396  [39202/60000]\n",
      "loss: 0.727374  [39402/60000]\n",
      "loss: 0.050977  [39602/60000]\n",
      "loss: 0.000071  [39802/60000]\n",
      "loss: 0.000450  [40002/60000]\n",
      "loss: 0.046192  [40202/60000]\n",
      "loss: 0.017012  [40402/60000]\n",
      "loss: 0.088085  [40602/60000]\n",
      "loss: 0.162846  [40802/60000]\n",
      "loss: 0.151113  [41002/60000]\n",
      "loss: 0.159837  [41202/60000]\n",
      "loss: 0.024482  [41402/60000]\n",
      "loss: 0.088366  [41602/60000]\n",
      "loss: 0.708721  [41802/60000]\n",
      "loss: 0.357031  [42002/60000]\n",
      "loss: 0.148215  [42202/60000]\n",
      "loss: 0.024094  [42402/60000]\n",
      "loss: 0.094646  [42602/60000]\n",
      "loss: 0.028758  [42802/60000]\n",
      "loss: 0.453392  [43002/60000]\n",
      "loss: 0.023188  [43202/60000]\n",
      "loss: 0.000000  [43402/60000]\n",
      "loss: 0.022460  [43602/60000]\n",
      "loss: 0.366308  [43802/60000]\n",
      "loss: 0.005815  [44002/60000]\n",
      "loss: 0.038763  [44202/60000]\n",
      "loss: 0.010530  [44402/60000]\n",
      "loss: 0.055917  [44602/60000]\n",
      "loss: -0.000000  [44802/60000]\n",
      "loss: 0.118667  [45002/60000]\n",
      "loss: 0.257758  [45202/60000]\n",
      "loss: 0.020302  [45402/60000]\n",
      "loss: 0.000187  [45602/60000]\n",
      "loss: 0.535510  [45802/60000]\n",
      "loss: 0.023414  [46002/60000]\n",
      "loss: 0.000046  [46202/60000]\n",
      "loss: 0.216716  [46402/60000]\n",
      "loss: 2.301048  [46602/60000]\n",
      "loss: 0.070539  [46802/60000]\n",
      "loss: 0.000557  [47002/60000]\n",
      "loss: 0.003376  [47202/60000]\n",
      "loss: 0.021525  [47402/60000]\n",
      "loss: 0.430928  [47602/60000]\n",
      "loss: 0.225665  [47802/60000]\n",
      "loss: 0.000046  [48002/60000]\n",
      "loss: 0.077790  [48202/60000]\n",
      "loss: 0.013544  [48402/60000]\n",
      "loss: 0.159500  [48602/60000]\n",
      "loss: 0.000031  [48802/60000]\n",
      "loss: 0.000049  [49002/60000]\n",
      "loss: 0.005628  [49202/60000]\n",
      "loss: 0.000030  [49402/60000]\n",
      "loss: 0.006237  [49602/60000]\n",
      "loss: 0.027649  [49802/60000]\n",
      "loss: 0.022387  [50002/60000]\n",
      "loss: 0.007883  [50202/60000]\n",
      "loss: 0.005176  [50402/60000]\n",
      "loss: 0.107248  [50602/60000]\n",
      "loss: 0.150059  [50802/60000]\n",
      "loss: 0.199830  [51002/60000]\n",
      "loss: 0.000005  [51202/60000]\n",
      "loss: 0.626367  [51402/60000]\n",
      "loss: 1.812353  [51602/60000]\n",
      "loss: 0.139678  [51802/60000]\n",
      "loss: 0.000017  [52002/60000]\n",
      "loss: 0.001147  [52202/60000]\n",
      "loss: 0.364599  [52402/60000]\n",
      "loss: 0.333798  [52602/60000]\n",
      "loss: 0.000705  [52802/60000]\n",
      "loss: 0.175129  [53002/60000]\n",
      "loss: 1.159186  [53202/60000]\n",
      "loss: 0.483606  [53402/60000]\n",
      "loss: 0.054357  [53602/60000]\n",
      "loss: 0.051786  [53802/60000]\n",
      "loss: 0.050634  [54002/60000]\n",
      "loss: 0.079810  [54202/60000]\n",
      "loss: 0.195348  [54402/60000]\n",
      "loss: 1.826363  [54602/60000]\n",
      "loss: 0.463342  [54802/60000]\n",
      "loss: 1.358126  [55002/60000]\n",
      "loss: -0.000000  [55202/60000]\n",
      "loss: 0.019320  [55402/60000]\n",
      "loss: 0.048504  [55602/60000]\n",
      "loss: 0.430617  [55802/60000]\n",
      "loss: 0.000010  [56002/60000]\n",
      "loss: 0.025022  [56202/60000]\n",
      "loss: 0.111352  [56402/60000]\n",
      "loss: 1.496525  [56602/60000]\n",
      "loss: 0.000778  [56802/60000]\n",
      "loss: 0.133630  [57002/60000]\n",
      "loss: 0.355864  [57202/60000]\n",
      "loss: 1.209955  [57402/60000]\n",
      "loss: 1.862889  [57602/60000]\n",
      "loss: 0.854641  [57802/60000]\n",
      "loss: 0.000876  [58002/60000]\n",
      "loss: 0.041663  [58202/60000]\n",
      "loss: 0.000035  [58402/60000]\n",
      "loss: 0.051425  [58602/60000]\n",
      "loss: 1.324726  [58802/60000]\n",
      "loss: 0.000034  [59002/60000]\n",
      "loss: 0.564986  [59202/60000]\n",
      "loss: 0.843647  [59402/60000]\n",
      "loss: 0.020209  [59602/60000]\n",
      "loss: 0.239734  [59802/60000]\n",
      "Test Error: Avg loss: 0.349270 Accuracy: 0.886100 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.000098  [    2/60000]\n",
      "loss: 0.104536  [  202/60000]\n",
      "loss: 0.000004  [  402/60000]\n",
      "loss: 0.117764  [  602/60000]\n",
      "loss: 0.027869  [  802/60000]\n",
      "loss: 0.073702  [ 1002/60000]\n",
      "loss: 0.351315  [ 1202/60000]\n",
      "loss: 0.199678  [ 1402/60000]\n",
      "loss: 3.068177  [ 1602/60000]\n",
      "loss: 0.160620  [ 1802/60000]\n",
      "loss: 0.031443  [ 2002/60000]\n",
      "loss: 0.000593  [ 2202/60000]\n",
      "loss: 0.002391  [ 2402/60000]\n",
      "loss: 0.289319  [ 2602/60000]\n",
      "loss: 0.000002  [ 2802/60000]\n",
      "loss: 0.066526  [ 3002/60000]\n",
      "loss: 0.332510  [ 3202/60000]\n",
      "loss: 0.508148  [ 3402/60000]\n",
      "loss: 0.002105  [ 3602/60000]\n",
      "loss: 0.000476  [ 3802/60000]\n",
      "loss: 0.000062  [ 4002/60000]\n",
      "loss: 0.000000  [ 4202/60000]\n",
      "loss: 0.130824  [ 4402/60000]\n",
      "loss: -0.000000  [ 4602/60000]\n",
      "loss: 0.879725  [ 4802/60000]\n",
      "loss: 0.408628  [ 5002/60000]\n",
      "loss: 0.002257  [ 5202/60000]\n",
      "loss: 0.347985  [ 5402/60000]\n",
      "loss: 0.002828  [ 5602/60000]\n",
      "loss: 0.073958  [ 5802/60000]\n",
      "loss: 3.115007  [ 6002/60000]\n",
      "loss: 0.000004  [ 6202/60000]\n",
      "loss: 0.078413  [ 6402/60000]\n",
      "loss: 0.060382  [ 6602/60000]\n",
      "loss: 0.000007  [ 6802/60000]\n",
      "loss: 2.930125  [ 7002/60000]\n",
      "loss: 3.238342  [ 7202/60000]\n",
      "loss: 0.734785  [ 7402/60000]\n",
      "loss: 1.939300  [ 7602/60000]\n",
      "loss: 0.405149  [ 7802/60000]\n",
      "loss: 0.000026  [ 8002/60000]\n",
      "loss: 0.000016  [ 8202/60000]\n",
      "loss: 0.000491  [ 8402/60000]\n",
      "loss: 0.131657  [ 8602/60000]\n",
      "loss: -0.000000  [ 8802/60000]\n",
      "loss: 0.001181  [ 9002/60000]\n",
      "loss: 0.404743  [ 9202/60000]\n",
      "loss: 0.030735  [ 9402/60000]\n",
      "loss: 0.001560  [ 9602/60000]\n",
      "loss: 0.066062  [ 9802/60000]\n",
      "loss: 0.000017  [10002/60000]\n",
      "loss: 0.005415  [10202/60000]\n",
      "loss: 0.321388  [10402/60000]\n",
      "loss: 0.000666  [10602/60000]\n",
      "loss: 0.259246  [10802/60000]\n",
      "loss: 0.000009  [11002/60000]\n",
      "loss: 0.035069  [11202/60000]\n",
      "loss: 0.000035  [11402/60000]\n",
      "loss: 0.325547  [11602/60000]\n",
      "loss: 0.000687  [11802/60000]\n",
      "loss: 0.075456  [12002/60000]\n",
      "loss: 0.088481  [12202/60000]\n",
      "loss: 0.062306  [12402/60000]\n",
      "loss: 0.000023  [12602/60000]\n",
      "loss: -0.000000  [12802/60000]\n",
      "loss: 0.103361  [13002/60000]\n",
      "loss: 0.288005  [13202/60000]\n",
      "loss: 1.375538  [13402/60000]\n",
      "loss: 0.374014  [13602/60000]\n",
      "loss: 0.296254  [13802/60000]\n",
      "loss: 0.026275  [14002/60000]\n",
      "loss: 0.854357  [14202/60000]\n",
      "loss: 0.001329  [14402/60000]\n",
      "loss: 0.056300  [14602/60000]\n",
      "loss: 0.531407  [14802/60000]\n",
      "loss: 0.067556  [15002/60000]\n",
      "loss: 0.000078  [15202/60000]\n",
      "loss: 0.000026  [15402/60000]\n",
      "loss: 0.000002  [15602/60000]\n",
      "loss: 0.001479  [15802/60000]\n",
      "loss: 1.748055  [16002/60000]\n",
      "loss: 0.352615  [16202/60000]\n",
      "loss: 0.046703  [16402/60000]\n",
      "loss: 0.152778  [16602/60000]\n",
      "loss: 0.747660  [16802/60000]\n",
      "loss: 0.012498  [17002/60000]\n",
      "loss: 0.000412  [17202/60000]\n",
      "loss: 0.009165  [17402/60000]\n",
      "loss: 0.008738  [17602/60000]\n",
      "loss: 0.673901  [17802/60000]\n",
      "loss: 0.018150  [18002/60000]\n",
      "loss: 0.267593  [18202/60000]\n",
      "loss: 0.029448  [18402/60000]\n",
      "loss: 0.000008  [18602/60000]\n",
      "loss: 0.333853  [18802/60000]\n",
      "loss: 0.528056  [19002/60000]\n",
      "loss: 0.014194  [19202/60000]\n",
      "loss: 0.017428  [19402/60000]\n",
      "loss: 0.004135  [19602/60000]\n",
      "loss: 0.203922  [19802/60000]\n",
      "loss: -0.000000  [20002/60000]\n",
      "loss: 0.288722  [20202/60000]\n",
      "loss: 0.188981  [20402/60000]\n",
      "loss: 0.309968  [20602/60000]\n",
      "loss: 1.191101  [20802/60000]\n",
      "loss: 0.094155  [21002/60000]\n",
      "loss: 0.000027  [21202/60000]\n",
      "loss: 0.056907  [21402/60000]\n",
      "loss: 0.111646  [21602/60000]\n",
      "loss: 0.389590  [21802/60000]\n",
      "loss: 0.000561  [22002/60000]\n",
      "loss: 0.027239  [22202/60000]\n",
      "loss: 0.000716  [22402/60000]\n",
      "loss: 0.000477  [22602/60000]\n",
      "loss: 0.076905  [22802/60000]\n",
      "loss: 0.217655  [23002/60000]\n",
      "loss: 1.064135  [23202/60000]\n",
      "loss: 1.042071  [23402/60000]\n",
      "loss: 0.000041  [23602/60000]\n",
      "loss: 0.003670  [23802/60000]\n",
      "loss: 0.032996  [24002/60000]\n",
      "loss: 0.000000  [24202/60000]\n",
      "loss: 0.006668  [24402/60000]\n",
      "loss: 0.328911  [24602/60000]\n",
      "loss: 0.000440  [24802/60000]\n",
      "loss: 0.000962  [25002/60000]\n",
      "loss: 0.000000  [25202/60000]\n",
      "loss: 0.189515  [25402/60000]\n",
      "loss: 0.846255  [25602/60000]\n",
      "loss: 2.425678  [25802/60000]\n",
      "loss: 0.352697  [26002/60000]\n",
      "loss: 0.773136  [26202/60000]\n",
      "loss: 0.075755  [26402/60000]\n",
      "loss: 0.479100  [26602/60000]\n",
      "loss: -0.000000  [26802/60000]\n",
      "loss: -0.000000  [27002/60000]\n",
      "loss: 0.000019  [27202/60000]\n",
      "loss: 0.015485  [27402/60000]\n",
      "loss: 1.341157  [27602/60000]\n",
      "loss: 1.067745  [27802/60000]\n",
      "loss: 0.087710  [28002/60000]\n",
      "loss: -0.000000  [28202/60000]\n",
      "loss: 0.084279  [28402/60000]\n",
      "loss: 0.118376  [28602/60000]\n",
      "loss: 0.000000  [28802/60000]\n",
      "loss: 0.017257  [29002/60000]\n",
      "loss: 0.000790  [29202/60000]\n",
      "loss: 0.000032  [29402/60000]\n",
      "loss: 0.117026  [29602/60000]\n",
      "loss: 0.403302  [29802/60000]\n",
      "loss: 0.004285  [30002/60000]\n",
      "loss: 0.079422  [30202/60000]\n",
      "loss: 0.000001  [30402/60000]\n",
      "loss: 0.001336  [30602/60000]\n",
      "loss: 0.001180  [30802/60000]\n",
      "loss: 0.003516  [31002/60000]\n",
      "loss: 0.058020  [31202/60000]\n",
      "loss: 0.178471  [31402/60000]\n",
      "loss: 0.293980  [31602/60000]\n",
      "loss: 0.000932  [31802/60000]\n",
      "loss: 0.595851  [32002/60000]\n",
      "loss: 0.846113  [32202/60000]\n",
      "loss: -0.000000  [32402/60000]\n",
      "loss: 0.749440  [32602/60000]\n",
      "loss: 0.176821  [32802/60000]\n",
      "loss: 0.003614  [33002/60000]\n",
      "loss: 1.123776  [33202/60000]\n",
      "loss: 0.024333  [33402/60000]\n",
      "loss: 0.024876  [33602/60000]\n",
      "loss: 0.870544  [33802/60000]\n",
      "loss: 0.000023  [34002/60000]\n",
      "loss: 0.789193  [34202/60000]\n",
      "loss: 0.353159  [34402/60000]\n",
      "loss: 0.021205  [34602/60000]\n",
      "loss: -0.000000  [34802/60000]\n",
      "loss: 0.035516  [35002/60000]\n",
      "loss: 0.374231  [35202/60000]\n",
      "loss: 0.172089  [35402/60000]\n",
      "loss: 0.000001  [35602/60000]\n",
      "loss: 0.324610  [35802/60000]\n",
      "loss: 0.028318  [36002/60000]\n",
      "loss: 0.451913  [36202/60000]\n",
      "loss: 0.000274  [36402/60000]\n",
      "loss: 0.000087  [36602/60000]\n",
      "loss: 0.079531  [36802/60000]\n",
      "loss: 0.040081  [37002/60000]\n",
      "loss: 0.003319  [37202/60000]\n",
      "loss: 0.024702  [37402/60000]\n",
      "loss: 0.334701  [37602/60000]\n",
      "loss: 1.228034  [37802/60000]\n",
      "loss: 0.155732  [38002/60000]\n",
      "loss: 0.000029  [38202/60000]\n",
      "loss: 0.000082  [38402/60000]\n",
      "loss: 0.007834  [38602/60000]\n",
      "loss: 0.210860  [38802/60000]\n",
      "loss: 0.034131  [39002/60000]\n",
      "loss: 0.000898  [39202/60000]\n",
      "loss: 0.968044  [39402/60000]\n",
      "loss: 0.222059  [39602/60000]\n",
      "loss: -0.000000  [39802/60000]\n",
      "loss: 0.017616  [40002/60000]\n",
      "loss: 0.023941  [40202/60000]\n",
      "loss: 0.002632  [40402/60000]\n",
      "loss: 0.038327  [40602/60000]\n",
      "loss: 0.124002  [40802/60000]\n",
      "loss: 0.058031  [41002/60000]\n",
      "loss: 0.573557  [41202/60000]\n",
      "loss: 0.033059  [41402/60000]\n",
      "loss: 0.054075  [41602/60000]\n",
      "loss: 0.501332  [41802/60000]\n",
      "loss: 0.451836  [42002/60000]\n",
      "loss: 0.046534  [42202/60000]\n",
      "loss: 0.041415  [42402/60000]\n",
      "loss: 0.125979  [42602/60000]\n",
      "loss: 0.008863  [42802/60000]\n",
      "loss: 0.243075  [43002/60000]\n",
      "loss: 0.010093  [43202/60000]\n",
      "loss: -0.000000  [43402/60000]\n",
      "loss: 0.004593  [43602/60000]\n",
      "loss: 0.363547  [43802/60000]\n",
      "loss: 0.007823  [44002/60000]\n",
      "loss: 0.053650  [44202/60000]\n",
      "loss: 0.001750  [44402/60000]\n",
      "loss: 0.208567  [44602/60000]\n",
      "loss: -0.000000  [44802/60000]\n",
      "loss: 0.109512  [45002/60000]\n",
      "loss: 0.688387  [45202/60000]\n",
      "loss: 0.001398  [45402/60000]\n",
      "loss: -0.000000  [45602/60000]\n",
      "loss: 0.537722  [45802/60000]\n",
      "loss: 0.144888  [46002/60000]\n",
      "loss: 0.006790  [46202/60000]\n",
      "loss: 0.254749  [46402/60000]\n",
      "loss: 1.257810  [46602/60000]\n",
      "loss: 0.056744  [46802/60000]\n",
      "loss: 0.001104  [47002/60000]\n",
      "loss: 0.005007  [47202/60000]\n",
      "loss: 0.009430  [47402/60000]\n",
      "loss: 0.392539  [47602/60000]\n",
      "loss: 0.346269  [47802/60000]\n",
      "loss: 0.000016  [48002/60000]\n",
      "loss: 0.004094  [48202/60000]\n",
      "loss: 0.000522  [48402/60000]\n",
      "loss: 0.258182  [48602/60000]\n",
      "loss: 0.000000  [48802/60000]\n",
      "loss: 0.000010  [49002/60000]\n",
      "loss: 0.015347  [49202/60000]\n",
      "loss: -0.000000  [49402/60000]\n",
      "loss: 0.016630  [49602/60000]\n",
      "loss: 0.004675  [49802/60000]\n",
      "loss: 0.014041  [50002/60000]\n",
      "loss: 0.009125  [50202/60000]\n",
      "loss: 0.000589  [50402/60000]\n",
      "loss: 0.513118  [50602/60000]\n",
      "loss: 0.044945  [50802/60000]\n",
      "loss: 0.027693  [51002/60000]\n",
      "loss: 0.000161  [51202/60000]\n",
      "loss: 0.580872  [51402/60000]\n",
      "loss: 2.972254  [51602/60000]\n",
      "loss: 0.070393  [51802/60000]\n",
      "loss: 0.000024  [52002/60000]\n",
      "loss: 0.000130  [52202/60000]\n",
      "loss: 0.204059  [52402/60000]\n",
      "loss: 0.648386  [52602/60000]\n",
      "loss: 0.000506  [52802/60000]\n",
      "loss: 0.444663  [53002/60000]\n",
      "loss: 0.616616  [53202/60000]\n",
      "loss: 0.358075  [53402/60000]\n",
      "loss: 0.574152  [53602/60000]\n",
      "loss: 0.093496  [53802/60000]\n",
      "loss: 0.103551  [54002/60000]\n",
      "loss: 0.190764  [54202/60000]\n",
      "loss: 0.434381  [54402/60000]\n",
      "loss: 0.587933  [54602/60000]\n",
      "loss: 0.568625  [54802/60000]\n",
      "loss: 1.469678  [55002/60000]\n",
      "loss: -0.000000  [55202/60000]\n",
      "loss: 0.001488  [55402/60000]\n",
      "loss: 0.002439  [55602/60000]\n",
      "loss: 0.550258  [55802/60000]\n",
      "loss: 0.000457  [56002/60000]\n",
      "loss: 0.056033  [56202/60000]\n",
      "loss: 0.046519  [56402/60000]\n",
      "loss: 1.668706  [56602/60000]\n",
      "loss: 0.003920  [56802/60000]\n",
      "loss: 0.097535  [57002/60000]\n",
      "loss: 0.235911  [57202/60000]\n",
      "loss: 0.162033  [57402/60000]\n",
      "loss: 1.201022  [57602/60000]\n",
      "loss: 0.337774  [57802/60000]\n",
      "loss: 0.002101  [58002/60000]\n",
      "loss: 0.000037  [58202/60000]\n",
      "loss: 0.000000  [58402/60000]\n",
      "loss: 0.015230  [58602/60000]\n",
      "loss: 1.720420  [58802/60000]\n",
      "loss: 0.000001  [59002/60000]\n",
      "loss: 0.600336  [59202/60000]\n",
      "loss: 0.432003  [59402/60000]\n",
      "loss: 0.010791  [59602/60000]\n",
      "loss: 0.197847  [59802/60000]\n",
      "Test Error: Avg loss: 0.354867 Accuracy: 0.886900 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.002925  [    2/60000]\n",
      "loss: 0.000706  [  202/60000]\n",
      "loss: 0.000001  [  402/60000]\n",
      "loss: 0.320896  [  602/60000]\n",
      "loss: 0.000032  [  802/60000]\n",
      "loss: 0.021273  [ 1002/60000]\n",
      "loss: 0.870858  [ 1202/60000]\n",
      "loss: 0.141631  [ 1402/60000]\n",
      "loss: 1.739080  [ 1602/60000]\n",
      "loss: 0.084755  [ 1802/60000]\n",
      "loss: 0.097596  [ 2002/60000]\n",
      "loss: 0.000076  [ 2202/60000]\n",
      "loss: 0.001437  [ 2402/60000]\n",
      "loss: 0.511114  [ 2602/60000]\n",
      "loss: 0.002589  [ 2802/60000]\n",
      "loss: 0.001418  [ 3002/60000]\n",
      "loss: 0.089410  [ 3202/60000]\n",
      "loss: 0.614864  [ 3402/60000]\n",
      "loss: 0.000053  [ 3602/60000]\n",
      "loss: 0.005131  [ 3802/60000]\n",
      "loss: 0.002873  [ 4002/60000]\n",
      "loss: 0.000000  [ 4202/60000]\n",
      "loss: 0.039344  [ 4402/60000]\n",
      "loss: -0.000000  [ 4602/60000]\n",
      "loss: 0.347109  [ 4802/60000]\n",
      "loss: 0.200806  [ 5002/60000]\n",
      "loss: 0.000128  [ 5202/60000]\n",
      "loss: 0.474732  [ 5402/60000]\n",
      "loss: 0.001801  [ 5602/60000]\n",
      "loss: 0.001448  [ 5802/60000]\n",
      "loss: 0.002131  [ 6002/60000]\n",
      "loss: 0.098958  [ 6202/60000]\n",
      "loss: 0.046668  [ 6402/60000]\n",
      "loss: 1.235527  [ 6602/60000]\n",
      "loss: 0.000043  [ 6802/60000]\n",
      "loss: 2.868047  [ 7002/60000]\n",
      "loss: 1.880438  [ 7202/60000]\n",
      "loss: 0.490817  [ 7402/60000]\n",
      "loss: 0.340368  [ 7602/60000]\n",
      "loss: 0.134490  [ 7802/60000]\n",
      "loss: 0.000012  [ 8002/60000]\n",
      "loss: 0.000137  [ 8202/60000]\n",
      "loss: 0.005635  [ 8402/60000]\n",
      "loss: 0.018564  [ 8602/60000]\n",
      "loss: -0.000000  [ 8802/60000]\n",
      "loss: 0.000444  [ 9002/60000]\n",
      "loss: 0.309249  [ 9202/60000]\n",
      "loss: 0.000293  [ 9402/60000]\n",
      "loss: 0.001221  [ 9602/60000]\n",
      "loss: 0.160448  [ 9802/60000]\n",
      "loss: 2.434261  [10002/60000]\n",
      "loss: 0.061562  [10202/60000]\n",
      "loss: 0.830370  [10402/60000]\n",
      "loss: 0.003633  [10602/60000]\n",
      "loss: 0.353936  [10802/60000]\n",
      "loss: 0.000497  [11002/60000]\n",
      "loss: 0.003906  [11202/60000]\n",
      "loss: 0.000006  [11402/60000]\n",
      "loss: 0.053638  [11602/60000]\n",
      "loss: 0.001754  [11802/60000]\n",
      "loss: 0.044560  [12002/60000]\n",
      "loss: 0.086353  [12202/60000]\n",
      "loss: 0.009945  [12402/60000]\n",
      "loss: 0.000015  [12602/60000]\n",
      "loss: -0.000000  [12802/60000]\n",
      "loss: 0.002667  [13002/60000]\n",
      "loss: 0.221541  [13202/60000]\n",
      "loss: 1.259108  [13402/60000]\n",
      "loss: 0.617134  [13602/60000]\n",
      "loss: 0.009982  [13802/60000]\n",
      "loss: 0.003338  [14002/60000]\n",
      "loss: 0.329204  [14202/60000]\n",
      "loss: 0.063517  [14402/60000]\n",
      "loss: 0.010307  [14602/60000]\n",
      "loss: 0.313689  [14802/60000]\n",
      "loss: 0.050127  [15002/60000]\n",
      "loss: 0.010856  [15202/60000]\n",
      "loss: 0.000031  [15402/60000]\n",
      "loss: 0.000095  [15602/60000]\n",
      "loss: -0.000000  [15802/60000]\n",
      "loss: 1.329163  [16002/60000]\n",
      "loss: 0.019839  [16202/60000]\n",
      "loss: 0.001619  [16402/60000]\n",
      "loss: 0.223547  [16602/60000]\n",
      "loss: 0.464355  [16802/60000]\n",
      "loss: 0.189623  [17002/60000]\n",
      "loss: 0.023088  [17202/60000]\n",
      "loss: 0.037340  [17402/60000]\n",
      "loss: 0.001284  [17602/60000]\n",
      "loss: 0.677155  [17802/60000]\n",
      "loss: 0.031348  [18002/60000]\n",
      "loss: 0.219047  [18202/60000]\n",
      "loss: 0.015499  [18402/60000]\n",
      "loss: 0.000091  [18602/60000]\n",
      "loss: 0.684789  [18802/60000]\n",
      "loss: 0.513666  [19002/60000]\n",
      "loss: 0.111379  [19202/60000]\n",
      "loss: 0.327677  [19402/60000]\n",
      "loss: 0.035873  [19602/60000]\n",
      "loss: 0.192768  [19802/60000]\n",
      "loss: -0.000000  [20002/60000]\n",
      "loss: 0.235412  [20202/60000]\n",
      "loss: 0.259480  [20402/60000]\n",
      "loss: 0.806946  [20602/60000]\n",
      "loss: 1.467646  [20802/60000]\n",
      "loss: 0.065815  [21002/60000]\n",
      "loss: 0.000074  [21202/60000]\n",
      "loss: 0.149993  [21402/60000]\n",
      "loss: 0.136070  [21602/60000]\n",
      "loss: 0.085310  [21802/60000]\n",
      "loss: 0.000087  [22002/60000]\n",
      "loss: 0.029076  [22202/60000]\n",
      "loss: 0.000120  [22402/60000]\n",
      "loss: 0.000568  [22602/60000]\n",
      "loss: 0.009216  [22802/60000]\n",
      "loss: 0.042167  [23002/60000]\n",
      "loss: 0.641237  [23202/60000]\n",
      "loss: 0.907646  [23402/60000]\n",
      "loss: 0.000021  [23602/60000]\n",
      "loss: 0.001540  [23802/60000]\n",
      "loss: 0.032732  [24002/60000]\n",
      "loss: 0.000001  [24202/60000]\n",
      "loss: 0.074196  [24402/60000]\n",
      "loss: 0.172168  [24602/60000]\n",
      "loss: 0.002062  [24802/60000]\n",
      "loss: 0.054586  [25002/60000]\n",
      "loss: 0.000162  [25202/60000]\n",
      "loss: 0.224077  [25402/60000]\n",
      "loss: 0.610747  [25602/60000]\n",
      "loss: 2.005488  [25802/60000]\n",
      "loss: 0.869343  [26002/60000]\n",
      "loss: 0.083760  [26202/60000]\n",
      "loss: 0.225932  [26402/60000]\n",
      "loss: 0.320354  [26602/60000]\n",
      "loss: -0.000000  [26802/60000]\n",
      "loss: 0.000181  [27002/60000]\n",
      "loss: 0.000309  [27202/60000]\n",
      "loss: 0.023168  [27402/60000]\n",
      "loss: 0.942414  [27602/60000]\n",
      "loss: 0.000316  [27802/60000]\n",
      "loss: 0.028618  [28002/60000]\n",
      "loss: 0.000006  [28202/60000]\n",
      "loss: 0.098990  [28402/60000]\n",
      "loss: 0.006375  [28602/60000]\n",
      "loss: -0.000000  [28802/60000]\n",
      "loss: 0.021373  [29002/60000]\n",
      "loss: -0.000000  [29202/60000]\n",
      "loss: 0.000000  [29402/60000]\n",
      "loss: 0.646981  [29602/60000]\n",
      "loss: 0.264697  [29802/60000]\n",
      "loss: 0.000056  [30002/60000]\n",
      "loss: 0.480045  [30202/60000]\n",
      "loss: 0.000005  [30402/60000]\n",
      "loss: 0.001769  [30602/60000]\n",
      "loss: 0.000000  [30802/60000]\n",
      "loss: 0.000205  [31002/60000]\n",
      "loss: 0.061543  [31202/60000]\n",
      "loss: 0.185051  [31402/60000]\n",
      "loss: 0.486608  [31602/60000]\n",
      "loss: 0.001024  [31802/60000]\n",
      "loss: 0.676302  [32002/60000]\n",
      "loss: 0.453225  [32202/60000]\n",
      "loss: -0.000000  [32402/60000]\n",
      "loss: 0.922999  [32602/60000]\n",
      "loss: 0.372257  [32802/60000]\n",
      "loss: 0.000138  [33002/60000]\n",
      "loss: 0.881911  [33202/60000]\n",
      "loss: 0.015349  [33402/60000]\n",
      "loss: 0.089081  [33602/60000]\n",
      "loss: 1.076694  [33802/60000]\n",
      "loss: 0.000467  [34002/60000]\n",
      "loss: 0.561605  [34202/60000]\n",
      "loss: 0.278340  [34402/60000]\n",
      "loss: 0.068877  [34602/60000]\n",
      "loss: -0.000000  [34802/60000]\n",
      "loss: 0.010091  [35002/60000]\n",
      "loss: 0.446882  [35202/60000]\n",
      "loss: 0.002259  [35402/60000]\n",
      "loss: 0.000009  [35602/60000]\n",
      "loss: 0.397951  [35802/60000]\n",
      "loss: 0.066170  [36002/60000]\n",
      "loss: 0.161517  [36202/60000]\n",
      "loss: 0.000829  [36402/60000]\n",
      "loss: 0.000004  [36602/60000]\n",
      "loss: 0.180549  [36802/60000]\n",
      "loss: 0.083123  [37002/60000]\n",
      "loss: 0.331474  [37202/60000]\n",
      "loss: 0.036111  [37402/60000]\n",
      "loss: 0.462903  [37602/60000]\n",
      "loss: 0.371254  [37802/60000]\n",
      "loss: 0.109753  [38002/60000]\n",
      "loss: 0.005627  [38202/60000]\n",
      "loss: -0.000000  [38402/60000]\n",
      "loss: 0.013846  [38602/60000]\n",
      "loss: 0.042425  [38802/60000]\n",
      "loss: 0.017180  [39002/60000]\n",
      "loss: 0.000264  [39202/60000]\n",
      "loss: 0.292109  [39402/60000]\n",
      "loss: 1.049722  [39602/60000]\n",
      "loss: 0.001911  [39802/60000]\n",
      "loss: 0.079145  [40002/60000]\n",
      "loss: 0.067340  [40202/60000]\n",
      "loss: 0.006452  [40402/60000]\n",
      "loss: 0.031493  [40602/60000]\n",
      "loss: 0.056776  [40802/60000]\n",
      "loss: 0.096921  [41002/60000]\n",
      "loss: 0.232891  [41202/60000]\n",
      "loss: 0.049556  [41402/60000]\n",
      "loss: 0.004295  [41602/60000]\n",
      "loss: 0.363741  [41802/60000]\n",
      "loss: 1.226952  [42002/60000]\n",
      "loss: 0.014848  [42202/60000]\n",
      "loss: 0.006890  [42402/60000]\n",
      "loss: 0.062607  [42602/60000]\n",
      "loss: 0.024880  [42802/60000]\n",
      "loss: 0.277617  [43002/60000]\n",
      "loss: 0.000002  [43202/60000]\n",
      "loss: -0.000000  [43402/60000]\n",
      "loss: 0.084922  [43602/60000]\n",
      "loss: 0.227081  [43802/60000]\n",
      "loss: 0.000059  [44002/60000]\n",
      "loss: 0.030311  [44202/60000]\n",
      "loss: 0.005311  [44402/60000]\n",
      "loss: 0.141171  [44602/60000]\n",
      "loss: -0.000000  [44802/60000]\n",
      "loss: 0.103548  [45002/60000]\n",
      "loss: 0.225519  [45202/60000]\n",
      "loss: 0.046245  [45402/60000]\n",
      "loss: -0.000000  [45602/60000]\n",
      "loss: 0.468468  [45802/60000]\n",
      "loss: 0.042877  [46002/60000]\n",
      "loss: 0.002304  [46202/60000]\n",
      "loss: 0.507393  [46402/60000]\n",
      "loss: 1.372930  [46602/60000]\n",
      "loss: 0.055749  [46802/60000]\n",
      "loss: 0.006165  [47002/60000]\n",
      "loss: 0.018519  [47202/60000]\n",
      "loss: 0.033792  [47402/60000]\n",
      "loss: 0.488213  [47602/60000]\n",
      "loss: 0.504114  [47802/60000]\n",
      "loss: 0.000110  [48002/60000]\n",
      "loss: 0.009851  [48202/60000]\n",
      "loss: 0.014712  [48402/60000]\n",
      "loss: 0.092776  [48602/60000]\n",
      "loss: 0.000026  [48802/60000]\n",
      "loss: 0.011277  [49002/60000]\n",
      "loss: 0.036431  [49202/60000]\n",
      "loss: -0.000000  [49402/60000]\n",
      "loss: 0.016553  [49602/60000]\n",
      "loss: 0.001068  [49802/60000]\n",
      "loss: 0.020102  [50002/60000]\n",
      "loss: 0.034860  [50202/60000]\n",
      "loss: 0.001092  [50402/60000]\n",
      "loss: 0.002986  [50602/60000]\n",
      "loss: 0.031945  [50802/60000]\n",
      "loss: 0.000202  [51002/60000]\n",
      "loss: 0.000001  [51202/60000]\n",
      "loss: 0.286864  [51402/60000]\n",
      "loss: 3.099430  [51602/60000]\n",
      "loss: 0.005563  [51802/60000]\n",
      "loss: 0.000546  [52002/60000]\n",
      "loss: 0.001036  [52202/60000]\n",
      "loss: 0.198440  [52402/60000]\n",
      "loss: 0.532780  [52602/60000]\n",
      "loss: 0.006088  [52802/60000]\n",
      "loss: 0.556127  [53002/60000]\n",
      "loss: 1.467679  [53202/60000]\n",
      "loss: 0.236020  [53402/60000]\n",
      "loss: 0.147179  [53602/60000]\n",
      "loss: 1.194673  [53802/60000]\n",
      "loss: 0.040203  [54002/60000]\n",
      "loss: 0.184098  [54202/60000]\n",
      "loss: 0.651881  [54402/60000]\n",
      "loss: 0.605540  [54602/60000]\n",
      "loss: 1.222043  [54802/60000]\n",
      "loss: 1.035065  [55002/60000]\n",
      "loss: -0.000000  [55202/60000]\n",
      "loss: 0.023760  [55402/60000]\n",
      "loss: 0.020530  [55602/60000]\n",
      "loss: 0.143369  [55802/60000]\n",
      "loss: 0.000077  [56002/60000]\n",
      "loss: 0.124438  [56202/60000]\n",
      "loss: 0.183830  [56402/60000]\n",
      "loss: 0.739411  [56602/60000]\n",
      "loss: 0.009266  [56802/60000]\n",
      "loss: 0.171801  [57002/60000]\n",
      "loss: 0.355233  [57202/60000]\n",
      "loss: 0.182184  [57402/60000]\n",
      "loss: 0.705979  [57602/60000]\n",
      "loss: 0.585186  [57802/60000]\n",
      "loss: 0.020638  [58002/60000]\n",
      "loss: 0.000096  [58202/60000]\n",
      "loss: 0.000629  [58402/60000]\n",
      "loss: 0.002367  [58602/60000]\n",
      "loss: 1.617015  [58802/60000]\n",
      "loss: -0.000000  [59002/60000]\n",
      "loss: 0.238235  [59202/60000]\n",
      "loss: 0.309224  [59402/60000]\n",
      "loss: 0.010893  [59602/60000]\n",
      "loss: 0.367205  [59802/60000]\n",
      "Test Error: Avg loss: 0.405025 Accuracy: 0.883700 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.000008  [    2/60000]\n",
      "loss: -0.000000  [  202/60000]\n",
      "loss: 0.000177  [  402/60000]\n",
      "loss: 0.148336  [  602/60000]\n",
      "loss: 0.000062  [  802/60000]\n",
      "loss: 0.068812  [ 1002/60000]\n",
      "loss: 0.155393  [ 1202/60000]\n",
      "loss: 0.217935  [ 1402/60000]\n",
      "loss: 3.501461  [ 1602/60000]\n",
      "loss: 0.082145  [ 1802/60000]\n",
      "loss: 0.060552  [ 2002/60000]\n",
      "loss: 0.000006  [ 2202/60000]\n",
      "loss: 0.005282  [ 2402/60000]\n",
      "loss: 0.206955  [ 2602/60000]\n",
      "loss: 0.000560  [ 2802/60000]\n",
      "loss: 0.059563  [ 3002/60000]\n",
      "loss: 0.188480  [ 3202/60000]\n",
      "loss: 1.209410  [ 3402/60000]\n",
      "loss: 0.143262  [ 3602/60000]\n",
      "loss: 0.772612  [ 3802/60000]\n",
      "loss: 0.004137  [ 4002/60000]\n",
      "loss: -0.000000  [ 4202/60000]\n",
      "loss: 0.003380  [ 4402/60000]\n",
      "loss: -0.000000  [ 4602/60000]\n",
      "loss: 0.598671  [ 4802/60000]\n",
      "loss: 0.009209  [ 5002/60000]\n",
      "loss: 0.000011  [ 5202/60000]\n",
      "loss: 0.447304  [ 5402/60000]\n",
      "loss: 0.217049  [ 5602/60000]\n",
      "loss: 0.007774  [ 5802/60000]\n",
      "loss: 0.014838  [ 6002/60000]\n",
      "loss: 0.001395  [ 6202/60000]\n",
      "loss: 0.020282  [ 6402/60000]\n",
      "loss: 0.482698  [ 6602/60000]\n",
      "loss: 0.000000  [ 6802/60000]\n",
      "loss: 3.541737  [ 7002/60000]\n",
      "loss: 2.613811  [ 7202/60000]\n",
      "loss: 0.250493  [ 7402/60000]\n",
      "loss: 0.185687  [ 7602/60000]\n",
      "loss: 0.200119  [ 7802/60000]\n",
      "loss: 0.000003  [ 8002/60000]\n",
      "loss: 0.000045  [ 8202/60000]\n",
      "loss: 0.019768  [ 8402/60000]\n",
      "loss: 0.107302  [ 8602/60000]\n",
      "loss: 0.002664  [ 8802/60000]\n",
      "loss: 0.000035  [ 9002/60000]\n",
      "loss: 0.235434  [ 9202/60000]\n",
      "loss: 0.001260  [ 9402/60000]\n",
      "loss: 0.002002  [ 9602/60000]\n",
      "loss: 0.063472  [ 9802/60000]\n",
      "loss: 0.002958  [10002/60000]\n",
      "loss: 0.027352  [10202/60000]\n",
      "loss: 0.049391  [10402/60000]\n",
      "loss: 0.041332  [10602/60000]\n",
      "loss: 0.346723  [10802/60000]\n",
      "loss: 0.006601  [11002/60000]\n",
      "loss: 0.027924  [11202/60000]\n",
      "loss: 0.000004  [11402/60000]\n",
      "loss: 0.057307  [11602/60000]\n",
      "loss: 0.004717  [11802/60000]\n",
      "loss: 0.472331  [12002/60000]\n",
      "loss: 0.129933  [12202/60000]\n",
      "loss: 0.000473  [12402/60000]\n",
      "loss: 0.000000  [12602/60000]\n",
      "loss: -0.000000  [12802/60000]\n",
      "loss: 0.010075  [13002/60000]\n",
      "loss: 0.080394  [13202/60000]\n",
      "loss: 1.263491  [13402/60000]\n",
      "loss: 0.396437  [13602/60000]\n",
      "loss: 0.026367  [13802/60000]\n",
      "loss: 0.008667  [14002/60000]\n",
      "loss: 0.198084  [14202/60000]\n",
      "loss: 0.000006  [14402/60000]\n",
      "loss: 0.072210  [14602/60000]\n",
      "loss: 0.954284  [14802/60000]\n",
      "loss: 0.006805  [15002/60000]\n",
      "loss: 0.000283  [15202/60000]\n",
      "loss: -0.000000  [15402/60000]\n",
      "loss: -0.000000  [15602/60000]\n",
      "loss: 0.000003  [15802/60000]\n",
      "loss: 1.285195  [16002/60000]\n",
      "loss: 0.022514  [16202/60000]\n",
      "loss: 0.017856  [16402/60000]\n",
      "loss: 0.044558  [16602/60000]\n",
      "loss: 0.666922  [16802/60000]\n",
      "loss: 0.009241  [17002/60000]\n",
      "loss: 0.014241  [17202/60000]\n",
      "loss: 0.000215  [17402/60000]\n",
      "loss: 0.000243  [17602/60000]\n",
      "loss: 0.385558  [17802/60000]\n",
      "loss: 0.067939  [18002/60000]\n",
      "loss: 0.052453  [18202/60000]\n",
      "loss: 0.017304  [18402/60000]\n",
      "loss: 0.000007  [18602/60000]\n",
      "loss: 0.300807  [18802/60000]\n",
      "loss: 0.315265  [19002/60000]\n",
      "loss: 0.028691  [19202/60000]\n",
      "loss: 0.090651  [19402/60000]\n",
      "loss: 0.000084  [19602/60000]\n",
      "loss: 0.168253  [19802/60000]\n",
      "loss: -0.000000  [20002/60000]\n",
      "loss: 0.131553  [20202/60000]\n",
      "loss: 0.121576  [20402/60000]\n",
      "loss: 0.443878  [20602/60000]\n",
      "loss: 0.612955  [20802/60000]\n",
      "loss: 0.190145  [21002/60000]\n",
      "loss: -0.000000  [21202/60000]\n",
      "loss: 0.101565  [21402/60000]\n",
      "loss: 0.518637  [21602/60000]\n",
      "loss: 0.184591  [21802/60000]\n",
      "loss: 0.073766  [22002/60000]\n",
      "loss: 0.008377  [22202/60000]\n",
      "loss: 0.000080  [22402/60000]\n",
      "loss: 0.000835  [22602/60000]\n",
      "loss: 0.146163  [22802/60000]\n",
      "loss: 0.050923  [23002/60000]\n",
      "loss: 0.803514  [23202/60000]\n",
      "loss: 0.724888  [23402/60000]\n",
      "loss: -0.000000  [23602/60000]\n",
      "loss: 0.167661  [23802/60000]\n",
      "loss: 0.010405  [24002/60000]\n",
      "loss: 0.000000  [24202/60000]\n",
      "loss: 0.005017  [24402/60000]\n",
      "loss: 0.586609  [24602/60000]\n",
      "loss: 0.000008  [24802/60000]\n",
      "loss: 0.022419  [25002/60000]\n",
      "loss: 0.000171  [25202/60000]\n",
      "loss: 1.629551  [25402/60000]\n",
      "loss: 1.428776  [25602/60000]\n",
      "loss: 4.875016  [25802/60000]\n",
      "loss: 0.331085  [26002/60000]\n",
      "loss: 0.071554  [26202/60000]\n",
      "loss: 0.000404  [26402/60000]\n",
      "loss: 0.105783  [26602/60000]\n",
      "loss: -0.000000  [26802/60000]\n",
      "loss: -0.000000  [27002/60000]\n",
      "loss: 0.000000  [27202/60000]\n",
      "loss: 0.065023  [27402/60000]\n",
      "loss: 0.943337  [27602/60000]\n",
      "loss: 0.103725  [27802/60000]\n",
      "loss: 0.036258  [28002/60000]\n",
      "loss: -0.000000  [28202/60000]\n",
      "loss: 0.086929  [28402/60000]\n",
      "loss: 0.006695  [28602/60000]\n",
      "loss: -0.000000  [28802/60000]\n",
      "loss: 0.023784  [29002/60000]\n",
      "loss: 0.000004  [29202/60000]\n",
      "loss: 0.000055  [29402/60000]\n",
      "loss: 0.478078  [29602/60000]\n",
      "loss: 0.403435  [29802/60000]\n",
      "loss: 0.000872  [30002/60000]\n",
      "loss: 0.277778  [30202/60000]\n",
      "loss: 0.002025  [30402/60000]\n",
      "loss: 0.005673  [30602/60000]\n",
      "loss: 0.000000  [30802/60000]\n",
      "loss: 0.000014  [31002/60000]\n",
      "loss: 0.144373  [31202/60000]\n",
      "loss: 0.166995  [31402/60000]\n",
      "loss: 0.238678  [31602/60000]\n",
      "loss: 0.458141  [31802/60000]\n",
      "loss: 0.077221  [32002/60000]\n",
      "loss: 1.585414  [32202/60000]\n",
      "loss: 0.000001  [32402/60000]\n",
      "loss: 0.960493  [32602/60000]\n",
      "loss: 0.077109  [32802/60000]\n",
      "loss: 0.000244  [33002/60000]\n",
      "loss: 0.611570  [33202/60000]\n",
      "loss: 0.000593  [33402/60000]\n",
      "loss: 0.019223  [33602/60000]\n",
      "loss: 0.590631  [33802/60000]\n",
      "loss: 0.000404  [34002/60000]\n",
      "loss: 1.000961  [34202/60000]\n",
      "loss: 0.419518  [34402/60000]\n",
      "loss: 0.014943  [34602/60000]\n",
      "loss: -0.000000  [34802/60000]\n",
      "loss: 0.164216  [35002/60000]\n",
      "loss: 0.189455  [35202/60000]\n",
      "loss: 0.033956  [35402/60000]\n",
      "loss: 0.000000  [35602/60000]\n",
      "loss: 0.298653  [35802/60000]\n",
      "loss: 0.037747  [36002/60000]\n",
      "loss: 0.999932  [36202/60000]\n",
      "loss: 0.006975  [36402/60000]\n",
      "loss: 0.000001  [36602/60000]\n",
      "loss: 0.023892  [36802/60000]\n",
      "loss: 0.009788  [37002/60000]\n",
      "loss: 0.000093  [37202/60000]\n",
      "loss: 0.002677  [37402/60000]\n",
      "loss: 0.469174  [37602/60000]\n",
      "loss: 0.541914  [37802/60000]\n",
      "loss: 0.138468  [38002/60000]\n",
      "loss: 0.000660  [38202/60000]\n",
      "loss: 0.000069  [38402/60000]\n",
      "loss: 0.003579  [38602/60000]\n",
      "loss: 0.204108  [38802/60000]\n",
      "loss: 0.043551  [39002/60000]\n",
      "loss: 0.000331  [39202/60000]\n",
      "loss: 0.834454  [39402/60000]\n",
      "loss: 0.127222  [39602/60000]\n",
      "loss: -0.000000  [39802/60000]\n",
      "loss: 0.000413  [40002/60000]\n",
      "loss: 0.030719  [40202/60000]\n",
      "loss: 0.000908  [40402/60000]\n",
      "loss: 0.026507  [40602/60000]\n",
      "loss: 0.135661  [40802/60000]\n",
      "loss: 0.005073  [41002/60000]\n",
      "loss: 0.408375  [41202/60000]\n",
      "loss: 0.094787  [41402/60000]\n",
      "loss: 0.005823  [41602/60000]\n",
      "loss: 0.145924  [41802/60000]\n",
      "loss: 0.678532  [42002/60000]\n",
      "loss: 0.018091  [42202/60000]\n",
      "loss: 0.000328  [42402/60000]\n",
      "loss: 0.104357  [42602/60000]\n",
      "loss: 0.005694  [42802/60000]\n",
      "loss: 0.134357  [43002/60000]\n",
      "loss: 0.000001  [43202/60000]\n",
      "loss: -0.000000  [43402/60000]\n",
      "loss: 0.054695  [43602/60000]\n",
      "loss: 0.762058  [43802/60000]\n",
      "loss: 0.007040  [44002/60000]\n",
      "loss: 0.093919  [44202/60000]\n",
      "loss: 0.002094  [44402/60000]\n",
      "loss: 0.047997  [44602/60000]\n",
      "loss: 0.000000  [44802/60000]\n",
      "loss: 0.931458  [45002/60000]\n",
      "loss: 0.188605  [45202/60000]\n",
      "loss: 0.005965  [45402/60000]\n",
      "loss: -0.000000  [45602/60000]\n",
      "loss: 0.347107  [45802/60000]\n",
      "loss: 0.000082  [46002/60000]\n",
      "loss: 0.000096  [46202/60000]\n",
      "loss: 0.308143  [46402/60000]\n",
      "loss: 2.036424  [46602/60000]\n",
      "loss: 0.004810  [46802/60000]\n",
      "loss: -0.000000  [47002/60000]\n",
      "loss: 0.004200  [47202/60000]\n",
      "loss: 0.000133  [47402/60000]\n",
      "loss: 0.416742  [47602/60000]\n",
      "loss: 0.256917  [47802/60000]\n",
      "loss: -0.000000  [48002/60000]\n",
      "loss: 0.026670  [48202/60000]\n",
      "loss: 0.000710  [48402/60000]\n",
      "loss: 0.093504  [48602/60000]\n",
      "loss: -0.000000  [48802/60000]\n",
      "loss: 0.000067  [49002/60000]\n",
      "loss: 0.001001  [49202/60000]\n",
      "loss: -0.000000  [49402/60000]\n",
      "loss: 0.006600  [49602/60000]\n",
      "loss: 0.012546  [49802/60000]\n",
      "loss: 0.026943  [50002/60000]\n",
      "loss: 0.023415  [50202/60000]\n",
      "loss: 0.000095  [50402/60000]\n",
      "loss: 0.396522  [50602/60000]\n",
      "loss: 0.000624  [50802/60000]\n",
      "loss: 0.012239  [51002/60000]\n",
      "loss: -0.000000  [51202/60000]\n",
      "loss: 0.129748  [51402/60000]\n",
      "loss: 1.057927  [51602/60000]\n",
      "loss: 0.022547  [51802/60000]\n",
      "loss: -0.000000  [52002/60000]\n",
      "loss: 0.000012  [52202/60000]\n",
      "loss: 0.096446  [52402/60000]\n",
      "loss: 0.737712  [52602/60000]\n",
      "loss: 0.027253  [52802/60000]\n",
      "loss: 0.520047  [53002/60000]\n",
      "loss: 0.923293  [53202/60000]\n",
      "loss: 0.652569  [53402/60000]\n",
      "loss: 0.498348  [53602/60000]\n",
      "loss: 0.001187  [53802/60000]\n",
      "loss: 0.387453  [54002/60000]\n",
      "loss: 0.363984  [54202/60000]\n",
      "loss: 0.316379  [54402/60000]\n",
      "loss: 0.366298  [54602/60000]\n",
      "loss: 0.020673  [54802/60000]\n",
      "loss: 1.271871  [55002/60000]\n",
      "loss: -0.000000  [55202/60000]\n",
      "loss: 0.000793  [55402/60000]\n",
      "loss: 0.023835  [55602/60000]\n",
      "loss: 0.318530  [55802/60000]\n",
      "loss: 0.000060  [56002/60000]\n",
      "loss: 0.053638  [56202/60000]\n",
      "loss: 0.057963  [56402/60000]\n",
      "loss: 0.405753  [56602/60000]\n",
      "loss: 0.002084  [56802/60000]\n",
      "loss: 0.252442  [57002/60000]\n",
      "loss: 0.235314  [57202/60000]\n",
      "loss: 1.425008  [57402/60000]\n",
      "loss: 0.134082  [57602/60000]\n",
      "loss: 0.422582  [57802/60000]\n",
      "loss: 0.011263  [58002/60000]\n",
      "loss: 0.000001  [58202/60000]\n",
      "loss: 0.001535  [58402/60000]\n",
      "loss: 0.021476  [58602/60000]\n",
      "loss: 0.357161  [58802/60000]\n",
      "loss: -0.000000  [59002/60000]\n",
      "loss: 0.644198  [59202/60000]\n",
      "loss: 0.978205  [59402/60000]\n",
      "loss: 0.064229  [59602/60000]\n",
      "loss: 0.227852  [59802/60000]\n",
      "Test Error: Avg loss: 0.558943 Accuracy: 0.893300 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "size = torch.Size((1,28,28))\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "classifier = Classifier(size, [], num_classes, nn.ReLU(), nn.CrossEntropyLoss()).to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    classifier.train_network(train_dataloader, optimizer)\n",
    "    classifier.test(test_dataloader)\n",
    "print(\"Done!\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8S7ny1kzQgL",
    "outputId": "042aed46-6ea6-41a4-fbc5-98792b5edf86"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.351409  [   16/60000]\n",
      "loss: 0.743569  [ 1616/60000]\n",
      "loss: 0.634741  [ 3216/60000]\n",
      "loss: 0.477763  [ 4816/60000]\n",
      "loss: 0.388747  [ 6416/60000]\n",
      "loss: 0.735409  [ 8016/60000]\n",
      "loss: 0.495244  [ 9616/60000]\n",
      "loss: 0.162244  [11216/60000]\n",
      "loss: 0.587680  [12816/60000]\n",
      "loss: 0.231612  [14416/60000]\n",
      "loss: 0.804324  [16016/60000]\n",
      "loss: 0.335152  [17616/60000]\n",
      "loss: 0.155147  [19216/60000]\n",
      "loss: 0.581478  [20816/60000]\n",
      "loss: 0.536934  [22416/60000]\n",
      "loss: 0.588689  [24016/60000]\n",
      "loss: 0.388161  [25616/60000]\n",
      "loss: 0.483898  [27216/60000]\n",
      "loss: 0.296262  [28816/60000]\n",
      "loss: 0.250107  [30416/60000]\n",
      "loss: 0.584727  [32016/60000]\n",
      "loss: 0.138394  [33616/60000]\n",
      "loss: 0.442564  [35216/60000]\n",
      "loss: 0.276764  [36816/60000]\n",
      "loss: 0.050868  [38416/60000]\n",
      "loss: 0.269830  [40016/60000]\n",
      "loss: 0.390101  [41616/60000]\n",
      "loss: 0.420953  [43216/60000]\n",
      "loss: 0.320305  [44816/60000]\n",
      "loss: 0.250185  [46416/60000]\n",
      "loss: 0.089710  [48016/60000]\n",
      "loss: 0.413710  [49616/60000]\n",
      "loss: 0.062282  [51216/60000]\n",
      "loss: 0.267024  [52816/60000]\n",
      "loss: 0.568821  [54416/60000]\n",
      "loss: 0.212249  [56016/60000]\n",
      "loss: 0.699709  [57616/60000]\n",
      "loss: 0.329388  [59216/60000]\n",
      "Test Error: Avg loss: 0.303242 Accuracy: 0.892300 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.152593  [   16/60000]\n",
      "loss: 0.241137  [ 1616/60000]\n",
      "loss: 0.306012  [ 3216/60000]\n",
      "loss: 0.242878  [ 4816/60000]\n",
      "loss: 0.115396  [ 6416/60000]\n",
      "loss: 0.514069  [ 8016/60000]\n",
      "loss: 0.309858  [ 9616/60000]\n",
      "loss: 0.075350  [11216/60000]\n",
      "loss: 0.127800  [12816/60000]\n",
      "loss: 0.181207  [14416/60000]\n",
      "loss: 0.452725  [16016/60000]\n",
      "loss: 0.175135  [17616/60000]\n",
      "loss: 0.089573  [19216/60000]\n",
      "loss: 0.784313  [20816/60000]\n",
      "loss: 0.395978  [22416/60000]\n",
      "loss: 0.544609  [24016/60000]\n",
      "loss: 0.403674  [25616/60000]\n",
      "loss: 0.263988  [27216/60000]\n",
      "loss: 0.274860  [28816/60000]\n",
      "loss: 0.229930  [30416/60000]\n",
      "loss: 0.592050  [32016/60000]\n",
      "loss: 0.107406  [33616/60000]\n",
      "loss: 0.206711  [35216/60000]\n",
      "loss: 0.173102  [36816/60000]\n",
      "loss: 0.148068  [38416/60000]\n",
      "loss: 0.330014  [40016/60000]\n",
      "loss: 0.361788  [41616/60000]\n",
      "loss: 0.409183  [43216/60000]\n",
      "loss: 0.209619  [44816/60000]\n",
      "loss: 0.152155  [46416/60000]\n",
      "loss: 0.126920  [48016/60000]\n",
      "loss: 0.311986  [49616/60000]\n",
      "loss: 0.039045  [51216/60000]\n",
      "loss: 0.160564  [52816/60000]\n",
      "loss: 0.213750  [54416/60000]\n",
      "loss: 0.180318  [56016/60000]\n",
      "loss: 0.361993  [57616/60000]\n",
      "loss: 0.372015  [59216/60000]\n",
      "Test Error: Avg loss: 0.309123 Accuracy: 0.891300 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.028969  [   16/60000]\n",
      "loss: 0.099730  [ 1616/60000]\n",
      "loss: 0.262540  [ 3216/60000]\n",
      "loss: 0.129224  [ 4816/60000]\n",
      "loss: 0.063250  [ 6416/60000]\n",
      "loss: 0.506342  [ 8016/60000]\n",
      "loss: 0.346334  [ 9616/60000]\n",
      "loss: 0.086976  [11216/60000]\n",
      "loss: 0.070840  [12816/60000]\n",
      "loss: 0.107375  [14416/60000]\n",
      "loss: 0.504837  [16016/60000]\n",
      "loss: 0.156361  [17616/60000]\n",
      "loss: 0.050073  [19216/60000]\n",
      "loss: 0.673700  [20816/60000]\n",
      "loss: 0.506722  [22416/60000]\n",
      "loss: 0.300000  [24016/60000]\n",
      "loss: 0.278338  [25616/60000]\n",
      "loss: 0.173948  [27216/60000]\n",
      "loss: 0.151477  [28816/60000]\n",
      "loss: 0.195349  [30416/60000]\n",
      "loss: 0.282978  [32016/60000]\n",
      "loss: 0.154424  [33616/60000]\n",
      "loss: 0.211056  [35216/60000]\n",
      "loss: 0.130905  [36816/60000]\n",
      "loss: 0.023243  [38416/60000]\n",
      "loss: 0.124616  [40016/60000]\n",
      "loss: 0.311421  [41616/60000]\n",
      "loss: 0.154803  [43216/60000]\n",
      "loss: 0.181131  [44816/60000]\n",
      "loss: 0.105903  [46416/60000]\n",
      "loss: 0.188292  [48016/60000]\n",
      "loss: 0.203075  [49616/60000]\n",
      "loss: 0.032400  [51216/60000]\n",
      "loss: 0.044328  [52816/60000]\n",
      "loss: 0.200416  [54416/60000]\n",
      "loss: 0.140501  [56016/60000]\n",
      "loss: 0.076384  [57616/60000]\n",
      "loss: 0.345089  [59216/60000]\n",
      "Test Error: Avg loss: 0.263491 Accuracy: 0.909600 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.031256  [   16/60000]\n",
      "loss: 0.240484  [ 1616/60000]\n",
      "loss: 0.178212  [ 3216/60000]\n",
      "loss: 0.138199  [ 4816/60000]\n",
      "loss: 0.025297  [ 6416/60000]\n",
      "loss: 0.557247  [ 8016/60000]\n",
      "loss: 0.245761  [ 9616/60000]\n",
      "loss: 0.142789  [11216/60000]\n",
      "loss: 0.038846  [12816/60000]\n",
      "loss: 0.137695  [14416/60000]\n",
      "loss: 0.454833  [16016/60000]\n",
      "loss: 0.058857  [17616/60000]\n",
      "loss: 0.031958  [19216/60000]\n",
      "loss: 0.426733  [20816/60000]\n",
      "loss: 0.402332  [22416/60000]\n",
      "loss: 0.302838  [24016/60000]\n",
      "loss: 0.245414  [25616/60000]\n",
      "loss: 0.201040  [27216/60000]\n",
      "loss: 0.184166  [28816/60000]\n",
      "loss: 0.206810  [30416/60000]\n",
      "loss: 0.261759  [32016/60000]\n",
      "loss: 0.045552  [33616/60000]\n",
      "loss: 0.106207  [35216/60000]\n",
      "loss: 0.050382  [36816/60000]\n",
      "loss: 0.016173  [38416/60000]\n",
      "loss: 0.218080  [40016/60000]\n",
      "loss: 0.227351  [41616/60000]\n",
      "loss: 0.185557  [43216/60000]\n",
      "loss: 0.181636  [44816/60000]\n",
      "loss: 0.078849  [46416/60000]\n",
      "loss: 0.127593  [48016/60000]\n",
      "loss: 0.230165  [49616/60000]\n",
      "loss: 0.014577  [51216/60000]\n",
      "loss: 0.057310  [52816/60000]\n",
      "loss: 0.083358  [54416/60000]\n",
      "loss: 0.129507  [56016/60000]\n",
      "loss: 0.071929  [57616/60000]\n",
      "loss: 0.325778  [59216/60000]\n",
      "Test Error: Avg loss: 0.261131 Accuracy: 0.908900 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.055465  [   16/60000]\n",
      "loss: 0.394901  [ 1616/60000]\n",
      "loss: 0.143516  [ 3216/60000]\n",
      "loss: 0.077091  [ 4816/60000]\n",
      "loss: 0.024317  [ 6416/60000]\n",
      "loss: 0.369875  [ 8016/60000]\n",
      "loss: 0.125793  [ 9616/60000]\n",
      "loss: 0.121498  [11216/60000]\n",
      "loss: 0.095048  [12816/60000]\n",
      "loss: 0.090481  [14416/60000]\n",
      "loss: 0.390042  [16016/60000]\n",
      "loss: 0.114371  [17616/60000]\n",
      "loss: 0.055230  [19216/60000]\n",
      "loss: 0.769457  [20816/60000]\n",
      "loss: 0.347225  [22416/60000]\n",
      "loss: 0.227542  [24016/60000]\n",
      "loss: 0.396654  [25616/60000]\n",
      "loss: 0.229530  [27216/60000]\n",
      "loss: 0.128771  [28816/60000]\n",
      "loss: 0.190993  [30416/60000]\n",
      "loss: 0.186678  [32016/60000]\n",
      "loss: 0.098363  [33616/60000]\n",
      "loss: 0.083686  [35216/60000]\n",
      "loss: 0.112462  [36816/60000]\n",
      "loss: 0.030796  [38416/60000]\n",
      "loss: 0.197848  [40016/60000]\n",
      "loss: 0.220307  [41616/60000]\n",
      "loss: 0.067457  [43216/60000]\n",
      "loss: 0.174871  [44816/60000]\n",
      "loss: 0.084059  [46416/60000]\n",
      "loss: 0.173131  [48016/60000]\n",
      "loss: 0.077960  [49616/60000]\n",
      "loss: 0.035267  [51216/60000]\n",
      "loss: 0.088960  [52816/60000]\n",
      "loss: 0.046979  [54416/60000]\n",
      "loss: 0.173031  [56016/60000]\n",
      "loss: 0.087779  [57616/60000]\n",
      "loss: 0.335510  [59216/60000]\n",
      "Test Error: Avg loss: 0.279163 Accuracy: 0.912100 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "size = torch.Size((1,28,28))\n",
    "\n",
    "\n",
    "classifier = Classifier(size, [], num_classes, nn.ReLU(), nn.CrossEntropyLoss()).to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    classifier.train_network(train_dataloader, optimizer)\n",
    "    classifier.test(test_dataloader)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgmMwLQ8zQgM",
    "outputId": "26c15f1a-dc81-4c88-914c-7c327fba2590"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.330386  [   32/60000]\n",
      "loss: 0.646023  [ 3232/60000]\n",
      "loss: 0.403816  [ 6432/60000]\n",
      "loss: 0.573532  [ 9632/60000]\n",
      "loss: 0.375540  [12832/60000]\n",
      "loss: 0.621366  [16032/60000]\n",
      "loss: 0.382900  [19232/60000]\n",
      "loss: 0.260836  [22432/60000]\n",
      "loss: 0.441003  [25632/60000]\n",
      "loss: 0.409191  [28832/60000]\n",
      "loss: 0.490514  [32032/60000]\n",
      "loss: 0.410852  [35232/60000]\n",
      "loss: 0.329305  [38432/60000]\n",
      "loss: 0.355550  [41632/60000]\n",
      "loss: 0.513526  [44832/60000]\n",
      "loss: 0.291051  [48032/60000]\n",
      "loss: 0.258055  [51232/60000]\n",
      "loss: 0.477261  [54432/60000]\n",
      "loss: 0.281667  [57632/60000]\n",
      "Test Error: Avg loss: 0.360336 Accuracy: 0.869200 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.178816  [   32/60000]\n",
      "loss: 0.341407  [ 3232/60000]\n",
      "loss: 0.174519  [ 6432/60000]\n",
      "loss: 0.291109  [ 9632/60000]\n",
      "loss: 0.169045  [12832/60000]\n",
      "loss: 0.432301  [16032/60000]\n",
      "loss: 0.277943  [19232/60000]\n",
      "loss: 0.248064  [22432/60000]\n",
      "loss: 0.306333  [25632/60000]\n",
      "loss: 0.283087  [28832/60000]\n",
      "loss: 0.454128  [32032/60000]\n",
      "loss: 0.311733  [35232/60000]\n",
      "loss: 0.309937  [38432/60000]\n",
      "loss: 0.325719  [41632/60000]\n",
      "loss: 0.340325  [44832/60000]\n",
      "loss: 0.351927  [48032/60000]\n",
      "loss: 0.131140  [51232/60000]\n",
      "loss: 0.251128  [54432/60000]\n",
      "loss: 0.315381  [57632/60000]\n",
      "Test Error: Avg loss: 0.274608 Accuracy: 0.899500 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.221150  [   32/60000]\n",
      "loss: 0.271082  [ 3232/60000]\n",
      "loss: 0.079754  [ 6432/60000]\n",
      "loss: 0.211023  [ 9632/60000]\n",
      "loss: 0.094880  [12832/60000]\n",
      "loss: 0.416337  [16032/60000]\n",
      "loss: 0.298758  [19232/60000]\n",
      "loss: 0.299252  [22432/60000]\n",
      "loss: 0.273439  [25632/60000]\n",
      "loss: 0.231934  [28832/60000]\n",
      "loss: 0.447186  [32032/60000]\n",
      "loss: 0.272340  [35232/60000]\n",
      "loss: 0.273606  [38432/60000]\n",
      "loss: 0.208789  [41632/60000]\n",
      "loss: 0.285014  [44832/60000]\n",
      "loss: 0.319959  [48032/60000]\n",
      "loss: 0.111667  [51232/60000]\n",
      "loss: 0.248961  [54432/60000]\n",
      "loss: 0.201788  [57632/60000]\n",
      "Test Error: Avg loss: 0.257927 Accuracy: 0.909900 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.122494  [   32/60000]\n",
      "loss: 0.180356  [ 3232/60000]\n",
      "loss: 0.108520  [ 6432/60000]\n",
      "loss: 0.198327  [ 9632/60000]\n",
      "loss: 0.079605  [12832/60000]\n",
      "loss: 0.293378  [16032/60000]\n",
      "loss: 0.264613  [19232/60000]\n",
      "loss: 0.224721  [22432/60000]\n",
      "loss: 0.237695  [25632/60000]\n",
      "loss: 0.174672  [28832/60000]\n",
      "loss: 0.441157  [32032/60000]\n",
      "loss: 0.244914  [35232/60000]\n",
      "loss: 0.258914  [38432/60000]\n",
      "loss: 0.187126  [41632/60000]\n",
      "loss: 0.363410  [44832/60000]\n",
      "loss: 0.348808  [48032/60000]\n",
      "loss: 0.104823  [51232/60000]\n",
      "loss: 0.087154  [54432/60000]\n",
      "loss: 0.174049  [57632/60000]\n",
      "Test Error: Avg loss: 0.260995 Accuracy: 0.907400 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.114650  [   32/60000]\n",
      "loss: 0.134475  [ 3232/60000]\n",
      "loss: 0.034382  [ 6432/60000]\n",
      "loss: 0.229142  [ 9632/60000]\n",
      "loss: 0.129250  [12832/60000]\n",
      "loss: 0.421314  [16032/60000]\n",
      "loss: 0.184854  [19232/60000]\n",
      "loss: 0.152347  [22432/60000]\n",
      "loss: 0.295543  [25632/60000]\n",
      "loss: 0.210748  [28832/60000]\n",
      "loss: 0.423189  [32032/60000]\n",
      "loss: 0.254735  [35232/60000]\n",
      "loss: 0.164795  [38432/60000]\n",
      "loss: 0.152332  [41632/60000]\n",
      "loss: 0.222191  [44832/60000]\n",
      "loss: 0.214872  [48032/60000]\n",
      "loss: 0.094049  [51232/60000]\n",
      "loss: 0.113161  [54432/60000]\n",
      "loss: 0.100420  [57632/60000]\n",
      "Test Error: Avg loss: 0.282587 Accuracy: 0.905300 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "size = torch.Size((1,28,28))\n",
    "\n",
    "\n",
    "classifier = Classifier(size, [], num_classes, nn.ReLU(), nn.CrossEntropyLoss()).to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    classifier.train_network(train_dataloader, optimizer)\n",
    "    classifier.test(test_dataloader)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcfI3UxmzQgM",
    "outputId": "31bfe979-8952-46e6-f27d-2e28abdc559d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.295302  [   64/60000]\n",
      "loss: 0.430262  [ 6464/60000]\n",
      "loss: 0.289053  [12864/60000]\n",
      "loss: 0.465057  [19264/60000]\n",
      "loss: 0.457087  [25664/60000]\n",
      "loss: 0.351757  [32064/60000]\n",
      "loss: 0.343226  [38464/60000]\n",
      "loss: 0.515788  [44864/60000]\n",
      "loss: 0.404897  [51264/60000]\n",
      "loss: 0.299663  [57664/60000]\n",
      "Test Error: Avg loss: 0.320517 Accuracy: 0.883300 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.143424  [   64/60000]\n",
      "loss: 0.402124  [ 6464/60000]\n",
      "loss: 0.172932  [12864/60000]\n",
      "loss: 0.314678  [19264/60000]\n",
      "loss: 0.486271  [25664/60000]\n",
      "loss: 0.356475  [32064/60000]\n",
      "loss: 0.265656  [38464/60000]\n",
      "loss: 0.329936  [44864/60000]\n",
      "loss: 0.291834  [51264/60000]\n",
      "loss: 0.213167  [57664/60000]\n",
      "Test Error: Avg loss: 0.365378 Accuracy: 0.865300 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.209710  [   64/60000]\n",
      "loss: 0.310169  [ 6464/60000]\n",
      "loss: 0.109971  [12864/60000]\n",
      "loss: 0.214783  [19264/60000]\n",
      "loss: 0.262748  [25664/60000]\n",
      "loss: 0.344636  [32064/60000]\n",
      "loss: 0.188091  [38464/60000]\n",
      "loss: 0.277695  [44864/60000]\n",
      "loss: 0.220641  [51264/60000]\n",
      "loss: 0.181517  [57664/60000]\n",
      "Test Error: Avg loss: 0.346402 Accuracy: 0.872000 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.142638  [   64/60000]\n",
      "loss: 0.193728  [ 6464/60000]\n",
      "loss: 0.131030  [12864/60000]\n",
      "loss: 0.246057  [19264/60000]\n",
      "loss: 0.235775  [25664/60000]\n",
      "loss: 0.278012  [32064/60000]\n",
      "loss: 0.173142  [38464/60000]\n",
      "loss: 0.239156  [44864/60000]\n",
      "loss: 0.194338  [51264/60000]\n",
      "loss: 0.168144  [57664/60000]\n",
      "Test Error: Avg loss: 0.317977 Accuracy: 0.888300 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.108194  [   64/60000]\n",
      "loss: 0.202096  [ 6464/60000]\n",
      "loss: 0.103428  [12864/60000]\n",
      "loss: 0.219621  [19264/60000]\n",
      "loss: 0.227102  [25664/60000]\n",
      "loss: 0.241905  [32064/60000]\n",
      "loss: 0.132529  [38464/60000]\n",
      "loss: 0.218739  [44864/60000]\n",
      "loss: 0.139240  [51264/60000]\n",
      "loss: 0.136323  [57664/60000]\n",
      "Test Error: Avg loss: 0.289061 Accuracy: 0.898400 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "\n",
    "size = torch.Size((1,28,28))\n",
    "\n",
    "\n",
    "classifier = Classifier(size, [], num_classes, nn.ReLU(), nn.CrossEntropyLoss()).to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    classifier.train_network(train_dataloader, optimizer)\n",
    "    classifier.test(test_dataloader)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "321goZHUzQgM",
    "outputId": "4079ddb0-071b-4038-fe7e-357aa99e94fd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([256, 1, 28, 28])\n",
      "Shape of y: torch.Size([256]) torch.int64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.320994  [  256/60000]\n",
      "loss: 0.342065  [25856/60000]\n",
      "loss: 0.329342  [51456/60000]\n",
      "Test Error: Avg loss: 0.360747 Accuracy: 0.869200 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.275018  [  256/60000]\n",
      "loss: 0.266455  [25856/60000]\n",
      "loss: 0.288956  [51456/60000]\n",
      "Test Error: Avg loss: 0.341543 Accuracy: 0.875200 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.236373  [  256/60000]\n",
      "loss: 0.221095  [25856/60000]\n",
      "loss: 0.261111  [51456/60000]\n",
      "Test Error: Avg loss: 0.321105 Accuracy: 0.886100 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.217084  [  256/60000]\n",
      "loss: 0.186538  [25856/60000]\n",
      "loss: 0.251768  [51456/60000]\n",
      "Test Error: Avg loss: 0.290621 Accuracy: 0.895800 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.172459  [  256/60000]\n",
      "loss: 0.189868  [25856/60000]\n",
      "loss: 0.207183  [51456/60000]\n",
      "Test Error: Avg loss: 0.281822 Accuracy: 0.898700 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "# Train Classifier (SGD)\n",
    "size = torch.Size((1,28,28))\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "classifier = Classifier(size, [], num_classes, nn.ReLU(), nn.CrossEntropyLoss()).to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    classifier.train_network(train_dataloader, optimizer)\n",
    "    classifier.test(test_dataloader)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXmz5WhOzQgN",
    "outputId": "a10b10df-1058-4c6e-9781-14e521aebd5a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([1024, 1, 28, 28])\n",
      "Shape of y: torch.Size([1024]) torch.int64\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.309053  [ 1024/60000]\n",
      "Test Error: Avg loss: 0.375022 Accuracy: 0.865000 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.334994  [ 1024/60000]\n",
      "Test Error: Avg loss: 0.312701 Accuracy: 0.890000 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.258511  [ 1024/60000]\n",
      "Test Error: Avg loss: 0.293876 Accuracy: 0.894000 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.236592  [ 1024/60000]\n",
      "Test Error: Avg loss: 0.264101 Accuracy: 0.905300 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.207143  [ 1024/60000]\n",
      "Test Error: Avg loss: 0.260746 Accuracy: 0.907300 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "# Train Classifier (SGD)\n",
    "size = torch.Size((1,28,28))\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "\n",
    "classifier = Classifier(size, [], num_classes, nn.ReLU(), nn.CrossEntropyLoss()).to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    classifier.train_network(train_dataloader, optimizer)\n",
    "    classifier.test(test_dataloader)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zv94eNfzQgN",
    "outputId": "68e97177-b7b8-4399-d67d-778f20866d9e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "gmhDD1uVzQgN"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
